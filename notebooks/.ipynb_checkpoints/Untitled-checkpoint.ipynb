{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a99f80a0-9bfe-4e16-80af-03fd600f15fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch_geometric\n",
    "from torch_geometric.utils import to_dense_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2df54b56-73fa-4c11-b0db-5e468ae091c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\t80013\n",
      "Test:\t1000\n",
      "Validation:\t19004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from RNARepLearn.datasets import Dataset_UTR5\n",
    "dataset = Dataset_UTR5(\"/lustre/groups/crna01/workspace/nicolas_msc/RNARepLearn/5utr/5UTR_design/processed/designed_library\")\n",
    "\n",
    "from RNARepLearn.utils import train_val_test_loaders\n",
    "train_loader, val_loader, test_loader = train_val_test_loaders(dataset, 0.8, 0.01, 0.19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ced3895-1996-4eb5-991b-991a5a147c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RNARepLearn.modules import RPINetEncoder, TE_Decoder\n",
    "layers = []\n",
    "layers.append(RPINetEncoder(4, 32, 3, 3))\n",
    "layers.append(TE_Decoder(32, 32))\n",
    "model = torch.nn.Sequential(*layers).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5de16cdf-ef00-4894-9101-eb29d38c92d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[1600, 4], edge_index=[2, 15005], edge_weight=[15005], mrl=[32], ID=[32], batch=[1600], ptr=[33])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch =next(iter(test_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23041ac4-af02-4253-a239-a717a5361eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/vnicolas.goedert/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/nn/modules/conv.py:303: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Convolution.cpp:882.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 4, 10], expected input[1, 1600, 5] to have 4 channels, but got 1600 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     15\u001b[0m cne \u001b[38;5;241m=\u001b[39m CNN_Encoder(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mcne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m, in \u001b[0;36mCNN_Encoder.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m      9\u001b[0m     x \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mx\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/nn/modules/conv.py:307\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/nn/modules/conv.py:303\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    301\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    302\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 4, 10], expected input[1, 1600, 5] to have 4 channels, but got 1600 channels instead"
     ]
    }
   ],
   "source": [
    "from torch.nn import Conv1d\n",
    "class CNN_Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv1d(input_channels, output_channels, kernel_size,padding='same')\n",
    "        self.conv2 = Conv1d(output_channels, output_channels, kernel_size,padding='same')\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        x_batched, fake_nodes_mask = to_dense_batch(batch.x, batch.batch)\n",
    "\n",
    "        x = self.conv1(torch.transpose(x_batched, 1, 2))\n",
    "        \n",
    "        \n",
    "        return x\n",
    "\n",
    "cne = CNN_Encoder(4,32,10)\n",
    "cne(next(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00737d51-dae1-41d9-8dd2-d461b7550c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new SummaryWriter\n"
     ]
    }
   ],
   "source": [
    "from RNARepLearn.train import TETraining\n",
    "training = TETraining(model, 5, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5895e9ad-8d71-43af-9ce4-e51d094a84aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training running on device: cuda\n",
      "[Epoch    1/   5] [Batch    1/1326] Loss:  8.47e+03 Validation-Loss:  6.80e+03\n",
      "[Epoch    1/   5] [Batch   11/1326] Loss:  1.39e+02 Validation-Loss:  3.01e+02\n",
      "[Epoch    1/   5] [Batch   21/1326] Loss:  1.49e+02 Validation-Loss:  1.71e+02\n",
      "[Epoch    1/   5] [Batch   31/1326] Loss:  6.92e+01 Validation-Loss:  7.86e+01\n",
      "[Epoch    1/   5] [Batch   41/1326] Loss:  4.81e+01 Validation-Loss:  8.07e+01\n",
      "[Epoch    1/   5] [Batch   51/1326] Loss:  7.67e+01 Validation-Loss:  9.43e+01\n",
      "[Epoch    1/   5] [Batch   61/1326] Loss:  7.92e+01 Validation-Loss:  6.94e+01\n",
      "[Epoch    1/   5] [Batch   71/1326] Loss:  6.29e+01 Validation-Loss:  4.82e+01\n",
      "[Epoch    1/   5] [Batch   81/1326] Loss:  9.84e+01 Validation-Loss:  5.52e+01\n",
      "[Epoch    1/   5] [Batch   91/1326] Loss:  9.73e+01 Validation-Loss:  7.16e+01\n",
      "[Epoch    1/   5] [Batch  101/1326] Loss:  4.81e+01 Validation-Loss:  8.61e+01\n",
      "[Epoch    1/   5] [Batch  111/1326] Loss:  5.19e+01 Validation-Loss:  4.49e+01\n",
      "[Epoch    1/   5] [Batch  121/1326] Loss:  7.20e+01 Validation-Loss:  7.25e+01\n",
      "[Epoch    1/   5] [Batch  131/1326] Loss:  6.38e+01 Validation-Loss:  4.71e+01\n",
      "[Epoch    1/   5] [Batch  141/1326] Loss:  8.17e+01 Validation-Loss:  8.18e+01\n",
      "[Epoch    1/   5] [Batch  151/1326] Loss:  1.15e+02 Validation-Loss:  8.13e+01\n",
      "[Epoch    1/   5] [Batch  161/1326] Loss:  8.08e+01 Validation-Loss:  4.36e+01\n",
      "[Epoch    1/   5] [Batch  171/1326] Loss:  7.02e+01 Validation-Loss:  1.13e+02\n",
      "[Epoch    1/   5] [Batch  181/1326] Loss:  1.50e+02 Validation-Loss:  9.19e+01\n",
      "[Epoch    1/   5] [Batch  191/1326] Loss:  9.66e+01 Validation-Loss:  9.90e+01\n",
      "[Epoch    1/   5] [Batch  201/1326] Loss:  8.74e+01 Validation-Loss:  3.13e+01\n",
      "[Epoch    1/   5] [Batch  211/1326] Loss:  1.00e+02 Validation-Loss:  8.72e+01\n",
      "[Epoch    1/   5] [Batch  221/1326] Loss:  7.94e+01 Validation-Loss:  1.02e+02\n",
      "[Epoch    1/   5] [Batch  231/1326] Loss:  6.24e+01 Validation-Loss:  8.60e+01\n",
      "[Epoch    1/   5] [Batch  241/1326] Loss:  6.75e+01 Validation-Loss:  7.25e+01\n",
      "[Epoch    1/   5] [Batch  251/1326] Loss:  9.67e+01 Validation-Loss:  9.51e+01\n",
      "[Epoch    1/   5] [Batch  261/1326] Loss:  9.36e+01 Validation-Loss:  6.53e+01\n",
      "[Epoch    1/   5] [Batch  271/1326] Loss:  1.32e+02 Validation-Loss:  4.38e+01\n",
      "[Epoch    1/   5] [Batch  281/1326] Loss:  5.08e+01 Validation-Loss:  9.13e+01\n",
      "[Epoch    1/   5] [Batch  291/1326] Loss:  4.37e+01 Validation-Loss:  7.71e+01\n",
      "[Epoch    1/   5] [Batch  301/1326] Loss:  6.09e+01 Validation-Loss:  9.92e+01\n",
      "[Epoch    1/   5] [Batch  311/1326] Loss:  8.17e+01 Validation-Loss:  7.29e+01\n",
      "[Epoch    1/   5] [Batch  321/1326] Loss:  8.16e+01 Validation-Loss:  5.71e+01\n",
      "[Epoch    1/   5] [Batch  331/1326] Loss:  6.51e+01 Validation-Loss:  7.89e+01\n",
      "[Epoch    1/   5] [Batch  341/1326] Loss:  1.15e+02 Validation-Loss:  6.92e+01\n",
      "[Epoch    1/   5] [Batch  351/1326] Loss:  4.22e+01 Validation-Loss:  7.13e+01\n",
      "[Epoch    1/   5] [Batch  361/1326] Loss:  6.40e+01 Validation-Loss:  8.85e+01\n",
      "[Epoch    1/   5] [Batch  371/1326] Loss:  7.09e+01 Validation-Loss:  9.91e+01\n",
      "[Epoch    1/   5] [Batch  381/1326] Loss:  1.22e+02 Validation-Loss:  8.42e+01\n",
      "[Epoch    1/   5] [Batch  391/1326] Loss:  5.48e+01 Validation-Loss:  7.97e+01\n",
      "[Epoch    1/   5] [Batch  401/1326] Loss:  6.12e+01 Validation-Loss:  8.81e+01\n",
      "[Epoch    1/   5] [Batch  411/1326] Loss:  6.59e+01 Validation-Loss:  5.93e+01\n",
      "[Epoch    1/   5] [Batch  421/1326] Loss:  1.03e+02 Validation-Loss:  1.01e+02\n",
      "[Epoch    1/   5] [Batch  431/1326] Loss:  5.58e+01 Validation-Loss:  4.41e+01\n",
      "[Epoch    1/   5] [Batch  441/1326] Loss:  1.05e+02 Validation-Loss:  3.14e+01\n",
      "[Epoch    1/   5] [Batch  451/1326] Loss:  7.26e+01 Validation-Loss:  7.01e+01\n",
      "[Epoch    1/   5] [Batch  461/1326] Loss:  1.18e+02 Validation-Loss:  8.35e+01\n",
      "[Epoch    1/   5] [Batch  471/1326] Loss:  7.70e+01 Validation-Loss:  1.02e+02\n",
      "[Epoch    1/   5] [Batch  481/1326] Loss:  9.51e+01 Validation-Loss:  5.70e+01\n",
      "[Epoch    1/   5] [Batch  491/1326] Loss:  6.59e+01 Validation-Loss:  7.44e+01\n",
      "[Epoch    1/   5] [Batch  501/1326] Loss:  7.08e+01 Validation-Loss:  5.47e+01\n",
      "[Epoch    1/   5] [Batch  511/1326] Loss:  5.77e+01 Validation-Loss:  8.69e+01\n",
      "[Epoch    1/   5] [Batch  521/1326] Loss:  8.21e+01 Validation-Loss:  6.86e+01\n",
      "[Epoch    1/   5] [Batch  531/1326] Loss:  8.20e+01 Validation-Loss:  1.06e+02\n",
      "[Epoch    1/   5] [Batch  541/1326] Loss:  6.94e+01 Validation-Loss:  1.27e+02\n",
      "[Epoch    1/   5] [Batch  551/1326] Loss:  9.92e+01 Validation-Loss:  8.30e+01\n",
      "[Epoch    1/   5] [Batch  561/1326] Loss:  6.76e+01 Validation-Loss:  9.11e+01\n",
      "[Epoch    1/   5] [Batch  571/1326] Loss:  2.86e+01 Validation-Loss:  1.00e+02\n",
      "[Epoch    1/   5] [Batch  581/1326] Loss:  3.95e+01 Validation-Loss:  6.02e+01\n",
      "[Epoch    1/   5] [Batch  591/1326] Loss:  8.85e+01 Validation-Loss:  7.59e+01\n",
      "[Epoch    1/   5] [Batch  601/1326] Loss:  8.15e+01 Validation-Loss:  9.93e+01\n",
      "[Epoch    1/   5] [Batch  611/1326] Loss:  1.28e+02 Validation-Loss:  6.19e+01\n",
      "[Epoch    1/   5] [Batch  621/1326] Loss:  1.61e+02 Validation-Loss:  5.47e+01\n",
      "[Epoch    1/   5] [Batch  631/1326] Loss:  7.74e+01 Validation-Loss:  7.96e+01\n",
      "[Epoch    1/   5] [Batch  641/1326] Loss:  7.11e+01 Validation-Loss:  8.30e+01\n",
      "[Epoch    1/   5] [Batch  651/1326] Loss:  1.05e+02 Validation-Loss:  8.26e+01\n",
      "[Epoch    1/   5] [Batch  661/1326] Loss:  4.71e+01 Validation-Loss:  1.03e+02\n",
      "[Epoch    1/   5] [Batch  671/1326] Loss:  9.70e+01 Validation-Loss:  1.06e+02\n",
      "[Epoch    1/   5] [Batch  681/1326] Loss:  6.09e+01 Validation-Loss:  6.54e+01\n",
      "[Epoch    1/   5] [Batch  691/1326] Loss:  7.90e+01 Validation-Loss:  4.50e+01\n",
      "[Epoch    1/   5] [Batch  701/1326] Loss:  8.82e+01 Validation-Loss:  7.26e+01\n",
      "[Epoch    1/   5] [Batch  711/1326] Loss:  8.51e+01 Validation-Loss:  8.07e+01\n",
      "[Epoch    1/   5] [Batch  721/1326] Loss:  7.46e+01 Validation-Loss:  6.79e+01\n",
      "[Epoch    1/   5] [Batch  731/1326] Loss:  5.07e+01 Validation-Loss:  1.07e+02\n",
      "[Epoch    1/   5] [Batch  741/1326] Loss:  8.75e+01 Validation-Loss:  8.48e+01\n",
      "[Epoch    1/   5] [Batch  751/1326] Loss:  5.74e+01 Validation-Loss:  8.33e+01\n",
      "[Epoch    1/   5] [Batch  761/1326] Loss:  7.09e+01 Validation-Loss:  5.38e+01\n",
      "[Epoch    1/   5] [Batch  771/1326] Loss:  8.69e+01 Validation-Loss:  9.31e+01\n",
      "[Epoch    1/   5] [Batch  781/1326] Loss:  8.23e+01 Validation-Loss:  5.96e+01\n",
      "[Epoch    1/   5] [Batch  791/1326] Loss:  8.10e+01 Validation-Loss:  1.00e+02\n",
      "[Epoch    1/   5] [Batch  801/1326] Loss:  6.89e+01 Validation-Loss:  3.37e+01\n",
      "[Epoch    1/   5] [Batch  811/1326] Loss:  7.53e+01 Validation-Loss:  1.12e+02\n",
      "[Epoch    1/   5] [Batch  821/1326] Loss:  1.10e+02 Validation-Loss:  8.29e+01\n",
      "[Epoch    1/   5] [Batch  831/1326] Loss:  7.99e+01 Validation-Loss:  5.84e+01\n",
      "[Epoch    1/   5] [Batch  841/1326] Loss:  8.98e+01 Validation-Loss:  1.02e+02\n",
      "[Epoch    1/   5] [Batch  851/1326] Loss:  9.30e+01 Validation-Loss:  7.39e+01\n",
      "[Epoch    1/   5] [Batch  861/1326] Loss:  6.71e+01 Validation-Loss:  8.65e+01\n",
      "[Epoch    1/   5] [Batch  871/1326] Loss:  8.05e+01 Validation-Loss:  6.99e+01\n",
      "[Epoch    1/   5] [Batch  881/1326] Loss:  9.81e+01 Validation-Loss:  8.43e+01\n",
      "[Epoch    1/   5] [Batch  891/1326] Loss:  9.05e+01 Validation-Loss:  6.87e+01\n",
      "[Epoch    1/   5] [Batch  901/1326] Loss:  9.81e+01 Validation-Loss:  7.47e+01\n",
      "[Epoch    1/   5] [Batch  911/1326] Loss:  9.06e+01 Validation-Loss:  1.06e+02\n",
      "[Epoch    1/   5] [Batch  921/1326] Loss:  9.34e+01 Validation-Loss:  6.00e+01\n",
      "[Epoch    1/   5] [Batch  931/1326] Loss:  1.01e+02 Validation-Loss:  7.43e+01\n",
      "[Epoch    1/   5] [Batch  941/1326] Loss:  7.85e+01 Validation-Loss:  1.73e+02\n",
      "[Epoch    1/   5] [Batch  951/1326] Loss:  1.03e+02 Validation-Loss:  7.76e+01\n",
      "[Epoch    1/   5] [Batch  961/1326] Loss:  9.32e+01 Validation-Loss:  7.54e+01\n",
      "[Epoch    1/   5] [Batch  971/1326] Loss:  7.34e+01 Validation-Loss:  5.64e+01\n",
      "[Epoch    1/   5] [Batch  981/1326] Loss:  1.17e+02 Validation-Loss:  1.25e+02\n",
      "[Epoch    1/   5] [Batch  991/1326] Loss:  1.98e+01 Validation-Loss:  7.97e+01\n",
      "[Epoch    1/   5] [Batch 1001/1326] Loss:  7.63e+01 Validation-Loss:  5.66e+01\n",
      "[Epoch    1/   5] [Batch 1011/1326] Loss:  1.05e+02 Validation-Loss:  4.59e+01\n",
      "[Epoch    1/   5] [Batch 1021/1326] Loss:  5.57e+01 Validation-Loss:  3.94e+01\n",
      "[Epoch    1/   5] [Batch 1031/1326] Loss:  8.85e+01 Validation-Loss:  1.13e+02\n",
      "[Epoch    1/   5] [Batch 1041/1326] Loss:  1.10e+02 Validation-Loss:  7.84e+01\n",
      "[Epoch    1/   5] [Batch 1051/1326] Loss:  9.62e+01 Validation-Loss:  8.52e+01\n",
      "[Epoch    1/   5] [Batch 1061/1326] Loss:  8.84e+01 Validation-Loss:  1.35e+02\n",
      "[Epoch    1/   5] [Batch 1071/1326] Loss:  7.65e+01 Validation-Loss:  7.40e+01\n",
      "[Epoch    1/   5] [Batch 1081/1326] Loss:  8.04e+01 Validation-Loss:  7.92e+01\n",
      "[Epoch    1/   5] [Batch 1091/1326] Loss:  1.08e+02 Validation-Loss:  6.08e+01\n",
      "[Epoch    1/   5] [Batch 1101/1326] Loss:  6.12e+01 Validation-Loss:  8.46e+01\n",
      "[Epoch    1/   5] [Batch 1111/1326] Loss:  1.17e+02 Validation-Loss:  4.78e+01\n",
      "[Epoch    1/   5] [Batch 1121/1326] Loss:  6.56e+01 Validation-Loss:  1.38e+02\n",
      "[Epoch    1/   5] [Batch 1131/1326] Loss:  6.20e+01 Validation-Loss:  1.16e+02\n",
      "[Epoch    1/   5] [Batch 1141/1326] Loss:  1.05e+02 Validation-Loss:  6.97e+01\n",
      "[Epoch    1/   5] [Batch 1151/1326] Loss:  6.39e+01 Validation-Loss:  8.62e+01\n",
      "[Epoch    1/   5] [Batch 1161/1326] Loss:  1.06e+02 Validation-Loss:  9.75e+01\n",
      "[Epoch    1/   5] [Batch 1171/1326] Loss:  7.84e+01 Validation-Loss:  1.16e+02\n",
      "[Epoch    1/   5] [Batch 1181/1326] Loss:  8.92e+01 Validation-Loss:  5.28e+01\n",
      "[Epoch    1/   5] [Batch 1191/1326] Loss:  6.12e+01 Validation-Loss:  6.88e+01\n",
      "[Epoch    1/   5] [Batch 1201/1326] Loss:  8.18e+01 Validation-Loss:  6.57e+01\n",
      "[Epoch    1/   5] [Batch 1211/1326] Loss:  5.86e+01 Validation-Loss:  7.11e+01\n",
      "[Epoch    1/   5] [Batch 1221/1326] Loss:  7.11e+01 Validation-Loss:  7.20e+01\n",
      "[Epoch    1/   5] [Batch 1231/1326] Loss:  6.37e+01 Validation-Loss:  6.63e+01\n",
      "[Epoch    1/   5] [Batch 1241/1326] Loss:  9.20e+01 Validation-Loss:  7.25e+01\n",
      "[Epoch    1/   5] [Batch 1251/1326] Loss:  7.89e+01 Validation-Loss:  7.78e+01\n",
      "[Epoch    1/   5] [Batch 1261/1326] Loss:  7.56e+01 Validation-Loss:  8.36e+01\n",
      "[Epoch    1/   5] [Batch 1271/1326] Loss:  3.83e+01 Validation-Loss:  1.08e+02\n",
      "[Epoch    1/   5] [Batch 1281/1326] Loss:  8.52e+01 Validation-Loss:  3.51e+01\n",
      "[Epoch    1/   5] [Batch 1291/1326] Loss:  6.46e+01 Validation-Loss:  4.40e+01\n",
      "[Epoch    1/   5] [Batch 1301/1326] Loss:  7.43e+01 Validation-Loss:  8.07e+01\n",
      "[Epoch    1/   5] [Batch 1311/1326] Loss:  7.51e+01 Validation-Loss:  8.53e+01\n",
      "[Epoch    1/   5] [Batch 1321/1326] Loss:  6.66e+01 Validation-Loss:  1.20e+02\n",
      "[Epoch    2/   5] [Batch    1/1326] Loss:  8.46e+01 Validation-Loss:  1.01e+02\n",
      "[Epoch    2/   5] [Batch   11/1326] Loss:  8.70e+01 Validation-Loss:  7.45e+01\n",
      "[Epoch    2/   5] [Batch   21/1326] Loss:  1.09e+02 Validation-Loss:  8.83e+01\n",
      "[Epoch    2/   5] [Batch   31/1326] Loss:  7.52e+01 Validation-Loss:  7.93e+01\n",
      "[Epoch    2/   5] [Batch   41/1326] Loss:  5.96e+01 Validation-Loss:  7.01e+01\n",
      "[Epoch    2/   5] [Batch   51/1326] Loss:  8.78e+01 Validation-Loss:  1.17e+02\n",
      "[Epoch    2/   5] [Batch   61/1326] Loss:  7.22e+01 Validation-Loss:  8.70e+01\n",
      "[Epoch    2/   5] [Batch   71/1326] Loss:  8.88e+01 Validation-Loss:  7.54e+01\n",
      "[Epoch    2/   5] [Batch   81/1326] Loss:  1.08e+02 Validation-Loss:  9.61e+01\n",
      "[Epoch    2/   5] [Batch   91/1326] Loss:  8.12e+01 Validation-Loss:  8.52e+01\n",
      "[Epoch    2/   5] [Batch  101/1326] Loss:  4.23e+01 Validation-Loss:  7.82e+01\n",
      "[Epoch    2/   5] [Batch  111/1326] Loss:  2.34e+01 Validation-Loss:  8.09e+01\n",
      "[Epoch    2/   5] [Batch  121/1326] Loss:  1.32e+02 Validation-Loss:  4.12e+01\n",
      "[Epoch    2/   5] [Batch  131/1326] Loss:  6.75e+01 Validation-Loss:  9.06e+01\n",
      "[Epoch    2/   5] [Batch  141/1326] Loss:  4.14e+01 Validation-Loss:  6.23e+01\n",
      "[Epoch    2/   5] [Batch  151/1326] Loss:  7.76e+01 Validation-Loss:  6.60e+01\n",
      "[Epoch    2/   5] [Batch  161/1326] Loss:  1.09e+02 Validation-Loss:  1.14e+02\n",
      "[Epoch    2/   5] [Batch  171/1326] Loss:  6.34e+01 Validation-Loss:  1.08e+02\n",
      "[Epoch    2/   5] [Batch  181/1326] Loss:  1.00e+02 Validation-Loss:  1.06e+02\n",
      "[Epoch    2/   5] [Batch  191/1326] Loss:  8.39e+01 Validation-Loss:  6.93e+01\n",
      "[Epoch    2/   5] [Batch  201/1326] Loss:  6.94e+01 Validation-Loss:  7.99e+01\n",
      "[Epoch    2/   5] [Batch  211/1326] Loss:  6.90e+01 Validation-Loss:  5.78e+01\n",
      "[Epoch    2/   5] [Batch  221/1326] Loss:  8.96e+01 Validation-Loss:  6.59e+01\n",
      "[Epoch    2/   5] [Batch  231/1326] Loss:  4.67e+01 Validation-Loss:  6.66e+01\n",
      "[Epoch    2/   5] [Batch  241/1326] Loss:  1.11e+02 Validation-Loss:  1.30e+02\n",
      "[Epoch    2/   5] [Batch  251/1326] Loss:  6.30e+01 Validation-Loss:  7.22e+01\n",
      "[Epoch    2/   5] [Batch  261/1326] Loss:  7.31e+01 Validation-Loss:  9.08e+01\n",
      "[Epoch    2/   5] [Batch  271/1326] Loss:  8.87e+01 Validation-Loss:  7.25e+01\n",
      "[Epoch    2/   5] [Batch  281/1326] Loss:  8.43e+01 Validation-Loss:  8.12e+01\n",
      "[Epoch    2/   5] [Batch  291/1326] Loss:  6.51e+01 Validation-Loss:  5.12e+01\n",
      "[Epoch    2/   5] [Batch  301/1326] Loss:  1.06e+02 Validation-Loss:  4.30e+01\n",
      "[Epoch    2/   5] [Batch  311/1326] Loss:  1.03e+02 Validation-Loss:  7.30e+01\n",
      "[Epoch    2/   5] [Batch  321/1326] Loss:  8.40e+01 Validation-Loss:  1.11e+02\n",
      "[Epoch    2/   5] [Batch  331/1326] Loss:  7.34e+01 Validation-Loss:  1.02e+02\n",
      "[Epoch    2/   5] [Batch  341/1326] Loss:  7.43e+01 Validation-Loss:  6.60e+01\n",
      "[Epoch    2/   5] [Batch  351/1326] Loss:  7.26e+01 Validation-Loss:  7.11e+01\n",
      "[Epoch    2/   5] [Batch  361/1326] Loss:  1.13e+02 Validation-Loss:  9.64e+01\n",
      "[Epoch    2/   5] [Batch  371/1326] Loss:  8.24e+01 Validation-Loss:  7.94e+01\n",
      "[Epoch    2/   5] [Batch  381/1326] Loss:  6.96e+01 Validation-Loss:  6.57e+01\n",
      "[Epoch    2/   5] [Batch  391/1326] Loss:  6.44e+01 Validation-Loss:  5.61e+01\n",
      "[Epoch    2/   5] [Batch  401/1326] Loss:  8.30e+01 Validation-Loss:  8.85e+01\n",
      "[Epoch    2/   5] [Batch  411/1326] Loss:  7.52e+01 Validation-Loss:  7.87e+01\n",
      "[Epoch    2/   5] [Batch  421/1326] Loss:  7.19e+01 Validation-Loss:  8.48e+01\n",
      "[Epoch    2/   5] [Batch  431/1326] Loss:  1.09e+02 Validation-Loss:  5.75e+01\n",
      "[Epoch    2/   5] [Batch  441/1326] Loss:  8.52e+01 Validation-Loss:  1.33e+02\n",
      "[Epoch    2/   5] [Batch  451/1326] Loss:  6.87e+01 Validation-Loss:  9.27e+01\n",
      "[Epoch    2/   5] [Batch  461/1326] Loss:  1.03e+02 Validation-Loss:  1.02e+02\n",
      "[Epoch    2/   5] [Batch  471/1326] Loss:  4.49e+01 Validation-Loss:  8.43e+01\n",
      "[Epoch    2/   5] [Batch  481/1326] Loss:  4.68e+01 Validation-Loss:  7.23e+01\n",
      "[Epoch    2/   5] [Batch  491/1326] Loss:  5.51e+01 Validation-Loss:  1.40e+02\n",
      "[Epoch    2/   5] [Batch  501/1326] Loss:  3.58e+01 Validation-Loss:  7.52e+01\n",
      "[Epoch    2/   5] [Batch  511/1326] Loss:  5.43e+01 Validation-Loss:  8.76e+01\n",
      "[Epoch    2/   5] [Batch  521/1326] Loss:  1.24e+02 Validation-Loss:  4.89e+01\n",
      "[Epoch    2/   5] [Batch  531/1326] Loss:  8.47e+01 Validation-Loss:  7.94e+01\n",
      "[Epoch    2/   5] [Batch  541/1326] Loss:  7.86e+01 Validation-Loss:  6.77e+01\n",
      "[Epoch    2/   5] [Batch  551/1326] Loss:  5.89e+01 Validation-Loss:  8.11e+01\n",
      "[Epoch    2/   5] [Batch  561/1326] Loss:  7.91e+01 Validation-Loss:  5.81e+01\n",
      "[Epoch    2/   5] [Batch  571/1326] Loss:  9.71e+01 Validation-Loss:  5.70e+01\n",
      "[Epoch    2/   5] [Batch  581/1326] Loss:  7.10e+01 Validation-Loss:  7.74e+01\n",
      "[Epoch    2/   5] [Batch  591/1326] Loss:  7.90e+01 Validation-Loss:  4.68e+01\n",
      "[Epoch    2/   5] [Batch  601/1326] Loss:  5.39e+01 Validation-Loss:  7.46e+01\n",
      "[Epoch    2/   5] [Batch  611/1326] Loss:  7.14e+01 Validation-Loss:  4.22e+01\n",
      "[Epoch    2/   5] [Batch  621/1326] Loss:  8.01e+01 Validation-Loss:  8.13e+01\n",
      "[Epoch    2/   5] [Batch  631/1326] Loss:  8.53e+01 Validation-Loss:  6.73e+01\n",
      "[Epoch    2/   5] [Batch  641/1326] Loss:  9.31e+01 Validation-Loss:  1.08e+02\n",
      "[Epoch    2/   5] [Batch  651/1326] Loss:  8.86e+01 Validation-Loss:  1.02e+02\n",
      "[Epoch    2/   5] [Batch  661/1326] Loss:  9.05e+01 Validation-Loss:  8.38e+01\n",
      "[Epoch    2/   5] [Batch  671/1326] Loss:  6.97e+01 Validation-Loss:  5.92e+01\n",
      "[Epoch    2/   5] [Batch  681/1326] Loss:  1.22e+02 Validation-Loss:  5.43e+01\n",
      "[Epoch    2/   5] [Batch  691/1326] Loss:  1.12e+02 Validation-Loss:  6.79e+01\n",
      "[Epoch    2/   5] [Batch  701/1326] Loss:  1.09e+02 Validation-Loss:  9.39e+01\n",
      "[Epoch    2/   5] [Batch  711/1326] Loss:  6.29e+01 Validation-Loss:  5.09e+01\n",
      "[Epoch    2/   5] [Batch  721/1326] Loss:  8.18e+01 Validation-Loss:  1.04e+02\n",
      "[Epoch    2/   5] [Batch  731/1326] Loss:  4.94e+01 Validation-Loss:  8.94e+01\n",
      "[Epoch    2/   5] [Batch  741/1326] Loss:  1.01e+02 Validation-Loss:  8.15e+01\n",
      "[Epoch    2/   5] [Batch  751/1326] Loss:  1.60e+02 Validation-Loss:  8.07e+01\n",
      "[Epoch    2/   5] [Batch  761/1326] Loss:  7.74e+01 Validation-Loss:  5.19e+01\n",
      "[Epoch    2/   5] [Batch  771/1326] Loss:  6.59e+01 Validation-Loss:  5.71e+01\n",
      "[Epoch    2/   5] [Batch  781/1326] Loss:  7.46e+01 Validation-Loss:  7.82e+01\n",
      "[Epoch    2/   5] [Batch  791/1326] Loss:  7.42e+01 Validation-Loss:  6.81e+01\n",
      "[Epoch    2/   5] [Batch  801/1326] Loss:  8.39e+01 Validation-Loss:  9.27e+01\n",
      "[Epoch    2/   5] [Batch  811/1326] Loss:  1.15e+02 Validation-Loss:  9.28e+01\n",
      "[Epoch    2/   5] [Batch  821/1326] Loss:  4.57e+01 Validation-Loss:  4.03e+01\n",
      "[Epoch    2/   5] [Batch  831/1326] Loss:  5.66e+01 Validation-Loss:  9.24e+01\n",
      "[Epoch    2/   5] [Batch  841/1326] Loss:  9.56e+01 Validation-Loss:  5.44e+01\n",
      "[Epoch    2/   5] [Batch  851/1326] Loss:  7.65e+01 Validation-Loss:  5.56e+01\n",
      "[Epoch    2/   5] [Batch  861/1326] Loss:  8.57e+01 Validation-Loss:  6.10e+01\n",
      "[Epoch    2/   5] [Batch  871/1326] Loss:  6.96e+01 Validation-Loss:  8.76e+01\n",
      "[Epoch    2/   5] [Batch  881/1326] Loss:  6.52e+01 Validation-Loss:  9.69e+01\n",
      "[Epoch    2/   5] [Batch  891/1326] Loss:  1.04e+02 Validation-Loss:  9.63e+01\n",
      "[Epoch    2/   5] [Batch  901/1326] Loss:  5.93e+01 Validation-Loss:  7.82e+01\n",
      "[Epoch    2/   5] [Batch  911/1326] Loss:  5.67e+01 Validation-Loss:  1.41e+02\n",
      "[Epoch    2/   5] [Batch  921/1326] Loss:  1.31e+02 Validation-Loss:  6.53e+01\n",
      "[Epoch    2/   5] [Batch  931/1326] Loss:  1.40e+02 Validation-Loss:  1.02e+02\n",
      "[Epoch    2/   5] [Batch  941/1326] Loss:  1.39e+02 Validation-Loss:  9.34e+01\n",
      "[Epoch    2/   5] [Batch  951/1326] Loss:  6.64e+01 Validation-Loss:  9.31e+01\n",
      "[Epoch    2/   5] [Batch  961/1326] Loss:  6.22e+01 Validation-Loss:  9.64e+01\n",
      "[Epoch    2/   5] [Batch  971/1326] Loss:  4.56e+01 Validation-Loss:  5.79e+01\n",
      "[Epoch    2/   5] [Batch  981/1326] Loss:  4.56e+01 Validation-Loss:  7.27e+01\n",
      "[Epoch    2/   5] [Batch  991/1326] Loss:  8.09e+01 Validation-Loss:  6.99e+01\n",
      "[Epoch    2/   5] [Batch 1001/1326] Loss:  4.04e+01 Validation-Loss:  6.91e+01\n",
      "[Epoch    2/   5] [Batch 1011/1326] Loss:  6.75e+01 Validation-Loss:  8.18e+01\n",
      "[Epoch    2/   5] [Batch 1021/1326] Loss:  8.12e+01 Validation-Loss:  7.40e+01\n",
      "[Epoch    2/   5] [Batch 1031/1326] Loss:  5.24e+01 Validation-Loss:  6.34e+01\n",
      "[Epoch    2/   5] [Batch 1041/1326] Loss:  5.41e+01 Validation-Loss:  9.60e+01\n",
      "[Epoch    2/   5] [Batch 1051/1326] Loss:  6.57e+01 Validation-Loss:  7.49e+01\n",
      "[Epoch    2/   5] [Batch 1061/1326] Loss:  1.02e+02 Validation-Loss:  5.34e+01\n",
      "[Epoch    2/   5] [Batch 1071/1326] Loss:  8.18e+01 Validation-Loss:  7.61e+01\n",
      "[Epoch    2/   5] [Batch 1081/1326] Loss:  8.00e+01 Validation-Loss:  7.39e+01\n",
      "[Epoch    2/   5] [Batch 1091/1326] Loss:  7.04e+01 Validation-Loss:  7.54e+01\n",
      "[Epoch    2/   5] [Batch 1101/1326] Loss:  5.21e+01 Validation-Loss:  6.92e+01\n",
      "[Epoch    2/   5] [Batch 1111/1326] Loss:  1.24e+02 Validation-Loss:  1.20e+02\n",
      "[Epoch    2/   5] [Batch 1121/1326] Loss:  8.90e+01 Validation-Loss:  1.02e+02\n",
      "[Epoch    2/   5] [Batch 1131/1326] Loss:  8.90e+01 Validation-Loss:  4.45e+01\n",
      "[Epoch    2/   5] [Batch 1141/1326] Loss:  1.27e+02 Validation-Loss:  1.23e+02\n",
      "[Epoch    2/   5] [Batch 1151/1326] Loss:  1.10e+02 Validation-Loss:  6.66e+01\n",
      "[Epoch    2/   5] [Batch 1161/1326] Loss:  1.00e+02 Validation-Loss:  3.07e+01\n",
      "[Epoch    2/   5] [Batch 1171/1326] Loss:  1.88e+02 Validation-Loss:  8.03e+01\n",
      "[Epoch    2/   5] [Batch 1181/1326] Loss:  8.33e+01 Validation-Loss:  1.12e+02\n",
      "[Epoch    2/   5] [Batch 1191/1326] Loss:  8.37e+01 Validation-Loss:  1.15e+02\n",
      "[Epoch    2/   5] [Batch 1201/1326] Loss:  1.08e+02 Validation-Loss:  7.59e+01\n",
      "[Epoch    2/   5] [Batch 1211/1326] Loss:  9.31e+01 Validation-Loss:  8.38e+01\n",
      "[Epoch    2/   5] [Batch 1221/1326] Loss:  5.30e+01 Validation-Loss:  6.46e+01\n",
      "[Epoch    2/   5] [Batch 1231/1326] Loss:  9.77e+01 Validation-Loss:  1.26e+02\n",
      "[Epoch    2/   5] [Batch 1241/1326] Loss:  7.72e+01 Validation-Loss:  1.28e+02\n",
      "[Epoch    2/   5] [Batch 1251/1326] Loss:  4.88e+01 Validation-Loss:  6.34e+01\n",
      "[Epoch    2/   5] [Batch 1261/1326] Loss:  1.07e+02 Validation-Loss:  1.48e+02\n",
      "[Epoch    2/   5] [Batch 1271/1326] Loss:  1.08e+02 Validation-Loss:  6.91e+01\n",
      "[Epoch    2/   5] [Batch 1281/1326] Loss:  8.13e+01 Validation-Loss:  8.85e+01\n",
      "[Epoch    2/   5] [Batch 1291/1326] Loss:  8.32e+01 Validation-Loss:  1.32e+02\n",
      "[Epoch    2/   5] [Batch 1301/1326] Loss:  8.18e+01 Validation-Loss:  6.98e+01\n",
      "[Epoch    2/   5] [Batch 1311/1326] Loss:  6.28e+01 Validation-Loss:  6.94e+01\n",
      "[Epoch    2/   5] [Batch 1321/1326] Loss:  8.79e+01 Validation-Loss:  7.61e+01\n",
      "[Epoch    3/   5] [Batch    1/1326] Loss:  6.72e+01 Validation-Loss:  7.28e+01\n",
      "[Epoch    3/   5] [Batch   11/1326] Loss:  8.21e+01 Validation-Loss:  9.09e+01\n",
      "[Epoch    3/   5] [Batch   21/1326] Loss:  6.30e+01 Validation-Loss:  8.52e+01\n",
      "[Epoch    3/   5] [Batch   31/1326] Loss:  8.65e+01 Validation-Loss:  7.67e+01\n",
      "[Epoch    3/   5] [Batch   41/1326] Loss:  9.55e+01 Validation-Loss:  1.07e+02\n",
      "[Epoch    3/   5] [Batch   51/1326] Loss:  7.21e+01 Validation-Loss:  9.54e+01\n",
      "[Epoch    3/   5] [Batch   61/1326] Loss:  8.99e+01 Validation-Loss:  5.59e+01\n",
      "[Epoch    3/   5] [Batch   71/1326] Loss:  1.12e+02 Validation-Loss:  1.14e+02\n",
      "[Epoch    3/   5] [Batch   81/1326] Loss:  5.91e+01 Validation-Loss:  8.00e+01\n",
      "[Epoch    3/   5] [Batch   91/1326] Loss:  1.05e+02 Validation-Loss:  9.48e+01\n",
      "[Epoch    3/   5] [Batch  101/1326] Loss:  8.75e+01 Validation-Loss:  8.03e+01\n",
      "[Epoch    3/   5] [Batch  111/1326] Loss:  6.55e+01 Validation-Loss:  8.11e+01\n",
      "[Epoch    3/   5] [Batch  121/1326] Loss:  5.46e+01 Validation-Loss:  8.92e+01\n",
      "[Epoch    3/   5] [Batch  131/1326] Loss:  8.32e+01 Validation-Loss:  1.12e+02\n",
      "[Epoch    3/   5] [Batch  141/1326] Loss:  4.90e+01 Validation-Loss:  1.19e+02\n",
      "[Epoch    3/   5] [Batch  151/1326] Loss:  5.50e+01 Validation-Loss:  8.53e+01\n",
      "[Epoch    3/   5] [Batch  161/1326] Loss:  9.44e+01 Validation-Loss:  7.87e+01\n",
      "[Epoch    3/   5] [Batch  171/1326] Loss:  4.88e+01 Validation-Loss:  7.07e+01\n",
      "[Epoch    3/   5] [Batch  181/1326] Loss:  5.78e+01 Validation-Loss:  8.01e+01\n",
      "[Epoch    3/   5] [Batch  191/1326] Loss:  5.52e+01 Validation-Loss:  5.20e+01\n",
      "[Epoch    3/   5] [Batch  201/1326] Loss:  9.06e+01 Validation-Loss:  7.84e+01\n",
      "[Epoch    3/   5] [Batch  211/1326] Loss:  8.33e+01 Validation-Loss:  6.89e+01\n",
      "[Epoch    3/   5] [Batch  221/1326] Loss:  9.26e+01 Validation-Loss:  8.42e+01\n",
      "[Epoch    3/   5] [Batch  231/1326] Loss:  8.88e+01 Validation-Loss:  9.26e+01\n",
      "[Epoch    3/   5] [Batch  241/1326] Loss:  1.03e+02 Validation-Loss:  5.56e+01\n",
      "[Epoch    3/   5] [Batch  251/1326] Loss:  8.42e+01 Validation-Loss:  5.90e+01\n",
      "[Epoch    3/   5] [Batch  261/1326] Loss:  6.89e+01 Validation-Loss:  1.22e+02\n",
      "[Epoch    3/   5] [Batch  271/1326] Loss:  7.16e+01 Validation-Loss:  1.28e+02\n",
      "[Epoch    3/   5] [Batch  281/1326] Loss:  8.88e+01 Validation-Loss:  6.93e+01\n",
      "[Epoch    3/   5] [Batch  291/1326] Loss:  8.11e+01 Validation-Loss:  4.53e+01\n",
      "[Epoch    3/   5] [Batch  301/1326] Loss:  1.31e+02 Validation-Loss:  7.02e+01\n",
      "[Epoch    3/   5] [Batch  311/1326] Loss:  8.69e+01 Validation-Loss:  1.01e+02\n",
      "[Epoch    3/   5] [Batch  321/1326] Loss:  1.31e+02 Validation-Loss:  5.03e+01\n",
      "[Epoch    3/   5] [Batch  331/1326] Loss:  6.95e+01 Validation-Loss:  8.26e+01\n",
      "[Epoch    3/   5] [Batch  341/1326] Loss:  6.31e+01 Validation-Loss:  8.93e+01\n",
      "[Epoch    3/   5] [Batch  351/1326] Loss:  5.66e+01 Validation-Loss:  7.87e+01\n",
      "[Epoch    3/   5] [Batch  361/1326] Loss:  5.63e+01 Validation-Loss:  4.37e+01\n",
      "[Epoch    3/   5] [Batch  371/1326] Loss:  9.96e+01 Validation-Loss:  1.05e+02\n",
      "[Epoch    3/   5] [Batch  381/1326] Loss:  8.69e+01 Validation-Loss:  1.02e+02\n",
      "[Epoch    3/   5] [Batch  391/1326] Loss:  5.05e+01 Validation-Loss:  1.04e+02\n",
      "[Epoch    3/   5] [Batch  401/1326] Loss:  7.21e+01 Validation-Loss:  7.81e+01\n",
      "[Epoch    3/   5] [Batch  411/1326] Loss:  7.57e+01 Validation-Loss:  6.60e+01\n",
      "[Epoch    3/   5] [Batch  421/1326] Loss:  7.00e+01 Validation-Loss:  1.16e+02\n",
      "[Epoch    3/   5] [Batch  431/1326] Loss:  1.19e+02 Validation-Loss:  9.39e+01\n",
      "[Epoch    3/   5] [Batch  441/1326] Loss:  6.16e+01 Validation-Loss:  6.75e+01\n",
      "[Epoch    3/   5] [Batch  451/1326] Loss:  5.01e+01 Validation-Loss:  1.24e+02\n",
      "[Epoch    3/   5] [Batch  461/1326] Loss:  7.87e+01 Validation-Loss:  1.07e+02\n",
      "[Epoch    3/   5] [Batch  471/1326] Loss:  7.38e+01 Validation-Loss:  5.40e+01\n",
      "[Epoch    3/   5] [Batch  481/1326] Loss:  8.71e+01 Validation-Loss:  8.87e+01\n",
      "[Epoch    3/   5] [Batch  491/1326] Loss:  9.99e+01 Validation-Loss:  1.03e+02\n",
      "[Epoch    3/   5] [Batch  501/1326] Loss:  8.51e+01 Validation-Loss:  1.07e+02\n",
      "[Epoch    3/   5] [Batch  511/1326] Loss:  9.89e+01 Validation-Loss:  9.53e+01\n",
      "[Epoch    3/   5] [Batch  521/1326] Loss:  1.05e+02 Validation-Loss:  9.32e+01\n",
      "[Epoch    3/   5] [Batch  531/1326] Loss:  6.47e+01 Validation-Loss:  6.56e+01\n",
      "[Epoch    3/   5] [Batch  541/1326] Loss:  4.71e+01 Validation-Loss:  7.12e+01\n",
      "[Epoch    3/   5] [Batch  551/1326] Loss:  1.02e+02 Validation-Loss:  7.62e+01\n",
      "[Epoch    3/   5] [Batch  561/1326] Loss:  8.38e+01 Validation-Loss:  1.29e+02\n",
      "[Epoch    3/   5] [Batch  571/1326] Loss:  7.94e+01 Validation-Loss:  1.10e+02\n",
      "[Epoch    3/   5] [Batch  581/1326] Loss:  7.21e+01 Validation-Loss:  9.10e+01\n",
      "[Epoch    3/   5] [Batch  591/1326] Loss:  7.04e+01 Validation-Loss:  9.45e+01\n",
      "[Epoch    3/   5] [Batch  601/1326] Loss:  9.17e+01 Validation-Loss:  1.04e+02\n",
      "[Epoch    3/   5] [Batch  611/1326] Loss:  4.82e+01 Validation-Loss:  1.21e+02\n",
      "[Epoch    3/   5] [Batch  621/1326] Loss:  6.99e+01 Validation-Loss:  7.88e+01\n",
      "[Epoch    3/   5] [Batch  631/1326] Loss:  6.37e+01 Validation-Loss:  1.08e+02\n",
      "[Epoch    3/   5] [Batch  641/1326] Loss:  7.55e+01 Validation-Loss:  8.85e+01\n",
      "[Epoch    3/   5] [Batch  651/1326] Loss:  9.00e+01 Validation-Loss:  9.82e+01\n",
      "[Epoch    3/   5] [Batch  661/1326] Loss:  5.19e+01 Validation-Loss:  6.98e+01\n",
      "[Epoch    3/   5] [Batch  671/1326] Loss:  6.07e+01 Validation-Loss:  6.01e+01\n",
      "[Epoch    3/   5] [Batch  681/1326] Loss:  6.49e+01 Validation-Loss:  5.61e+01\n",
      "[Epoch    3/   5] [Batch  691/1326] Loss:  1.30e+02 Validation-Loss:  1.09e+02\n",
      "[Epoch    3/   5] [Batch  701/1326] Loss:  3.56e+01 Validation-Loss:  7.01e+01\n",
      "[Epoch    3/   5] [Batch  711/1326] Loss:  8.46e+01 Validation-Loss:  3.87e+01\n",
      "[Epoch    3/   5] [Batch  721/1326] Loss:  4.82e+01 Validation-Loss:  1.14e+02\n",
      "[Epoch    3/   5] [Batch  731/1326] Loss:  4.64e+01 Validation-Loss:  6.39e+01\n",
      "[Epoch    3/   5] [Batch  741/1326] Loss:  1.19e+02 Validation-Loss:  6.17e+01\n",
      "[Epoch    3/   5] [Batch  751/1326] Loss:  4.54e+01 Validation-Loss:  8.57e+01\n",
      "[Epoch    3/   5] [Batch  761/1326] Loss:  1.11e+02 Validation-Loss:  8.99e+01\n",
      "[Epoch    3/   5] [Batch  771/1326] Loss:  7.56e+01 Validation-Loss:  1.26e+02\n",
      "[Epoch    3/   5] [Batch  781/1326] Loss:  4.30e+01 Validation-Loss:  8.16e+01\n",
      "[Epoch    3/   5] [Batch  791/1326] Loss:  1.13e+02 Validation-Loss:  1.00e+02\n",
      "[Epoch    3/   5] [Batch  801/1326] Loss:  7.07e+01 Validation-Loss:  3.61e+01\n",
      "[Epoch    3/   5] [Batch  811/1326] Loss:  6.19e+01 Validation-Loss:  7.68e+01\n",
      "[Epoch    3/   5] [Batch  821/1326] Loss:  1.03e+02 Validation-Loss:  1.05e+02\n",
      "[Epoch    3/   5] [Batch  831/1326] Loss:  8.02e+01 Validation-Loss:  5.98e+01\n",
      "[Epoch    3/   5] [Batch  841/1326] Loss:  1.21e+02 Validation-Loss:  6.14e+01\n",
      "[Epoch    3/   5] [Batch  851/1326] Loss:  9.29e+01 Validation-Loss:  8.45e+01\n",
      "[Epoch    3/   5] [Batch  861/1326] Loss:  6.69e+01 Validation-Loss:  7.32e+01\n",
      "[Epoch    3/   5] [Batch  871/1326] Loss:  6.41e+01 Validation-Loss:  7.32e+01\n",
      "[Epoch    3/   5] [Batch  881/1326] Loss:  8.47e+01 Validation-Loss:  4.91e+01\n",
      "[Epoch    3/   5] [Batch  891/1326] Loss:  7.98e+01 Validation-Loss:  7.46e+01\n",
      "[Epoch    3/   5] [Batch  901/1326] Loss:  8.53e+01 Validation-Loss:  9.39e+01\n",
      "[Epoch    3/   5] [Batch  911/1326] Loss:  5.14e+01 Validation-Loss:  3.78e+01\n",
      "[Epoch    3/   5] [Batch  921/1326] Loss:  7.15e+01 Validation-Loss:  1.51e+02\n",
      "[Epoch    3/   5] [Batch  931/1326] Loss:  7.21e+01 Validation-Loss:  1.26e+02\n",
      "[Epoch    3/   5] [Batch  941/1326] Loss:  8.82e+01 Validation-Loss:  8.71e+01\n",
      "[Epoch    3/   5] [Batch  951/1326] Loss:  9.15e+01 Validation-Loss:  9.19e+01\n",
      "[Epoch    3/   5] [Batch  961/1326] Loss:  8.57e+01 Validation-Loss:  8.66e+01\n",
      "[Epoch    3/   5] [Batch  971/1326] Loss:  8.75e+01 Validation-Loss:  6.02e+01\n",
      "[Epoch    3/   5] [Batch  981/1326] Loss:  6.88e+01 Validation-Loss:  8.50e+01\n",
      "[Epoch    3/   5] [Batch  991/1326] Loss:  8.39e+01 Validation-Loss:  7.40e+01\n",
      "[Epoch    3/   5] [Batch 1001/1326] Loss:  8.58e+01 Validation-Loss:  6.76e+01\n",
      "[Epoch    3/   5] [Batch 1011/1326] Loss:  7.16e+01 Validation-Loss:  6.84e+01\n",
      "[Epoch    3/   5] [Batch 1021/1326] Loss:  7.11e+01 Validation-Loss:  1.04e+02\n",
      "[Epoch    3/   5] [Batch 1031/1326] Loss:  7.71e+01 Validation-Loss:  9.46e+01\n",
      "[Epoch    3/   5] [Batch 1041/1326] Loss:  5.22e+01 Validation-Loss:  8.39e+01\n",
      "[Epoch    3/   5] [Batch 1051/1326] Loss:  6.49e+01 Validation-Loss:  9.35e+01\n",
      "[Epoch    3/   5] [Batch 1061/1326] Loss:  3.44e+01 Validation-Loss:  9.59e+01\n",
      "[Epoch    3/   5] [Batch 1071/1326] Loss:  8.96e+01 Validation-Loss:  1.85e+02\n",
      "[Epoch    3/   5] [Batch 1081/1326] Loss:  5.81e+01 Validation-Loss:  6.22e+01\n",
      "[Epoch    3/   5] [Batch 1091/1326] Loss:  7.70e+01 Validation-Loss:  9.84e+01\n",
      "[Epoch    3/   5] [Batch 1101/1326] Loss:  1.02e+02 Validation-Loss:  1.27e+02\n",
      "[Epoch    3/   5] [Batch 1111/1326] Loss:  1.06e+02 Validation-Loss:  7.93e+01\n",
      "[Epoch    3/   5] [Batch 1121/1326] Loss:  1.67e+02 Validation-Loss:  8.90e+01\n",
      "[Epoch    3/   5] [Batch 1131/1326] Loss:  7.67e+01 Validation-Loss:  9.22e+01\n",
      "[Epoch    3/   5] [Batch 1141/1326] Loss:  1.17e+02 Validation-Loss:  7.82e+01\n",
      "[Epoch    3/   5] [Batch 1151/1326] Loss:  7.86e+01 Validation-Loss:  7.38e+01\n",
      "[Epoch    3/   5] [Batch 1161/1326] Loss:  8.80e+01 Validation-Loss:  5.24e+01\n",
      "[Epoch    3/   5] [Batch 1171/1326] Loss:  1.16e+02 Validation-Loss:  1.09e+02\n",
      "[Epoch    3/   5] [Batch 1181/1326] Loss:  6.64e+01 Validation-Loss:  1.03e+02\n",
      "[Epoch    3/   5] [Batch 1191/1326] Loss:  5.65e+01 Validation-Loss:  6.62e+01\n",
      "[Epoch    3/   5] [Batch 1201/1326] Loss:  8.38e+01 Validation-Loss:  1.06e+02\n",
      "[Epoch    3/   5] [Batch 1211/1326] Loss:  7.83e+01 Validation-Loss:  5.87e+01\n",
      "[Epoch    3/   5] [Batch 1221/1326] Loss:  4.61e+01 Validation-Loss:  5.99e+01\n",
      "[Epoch    3/   5] [Batch 1231/1326] Loss:  8.22e+01 Validation-Loss:  1.50e+02\n",
      "[Epoch    3/   5] [Batch 1241/1326] Loss:  7.71e+01 Validation-Loss:  8.94e+01\n",
      "[Epoch    3/   5] [Batch 1251/1326] Loss:  5.33e+01 Validation-Loss:  6.97e+01\n",
      "[Epoch    3/   5] [Batch 1261/1326] Loss:  5.73e+01 Validation-Loss:  1.10e+02\n",
      "[Epoch    3/   5] [Batch 1271/1326] Loss:  9.37e+01 Validation-Loss:  8.60e+01\n",
      "[Epoch    3/   5] [Batch 1281/1326] Loss:  8.79e+01 Validation-Loss:  7.14e+01\n",
      "[Epoch    3/   5] [Batch 1291/1326] Loss:  1.06e+02 Validation-Loss:  9.36e+01\n",
      "[Epoch    3/   5] [Batch 1301/1326] Loss:  7.49e+01 Validation-Loss:  5.17e+01\n",
      "[Epoch    3/   5] [Batch 1311/1326] Loss:  9.73e+01 Validation-Loss:  1.03e+02\n",
      "[Epoch    3/   5] [Batch 1321/1326] Loss:  1.25e+02 Validation-Loss:  1.24e+02\n",
      "[Epoch    4/   5] [Batch    1/1326] Loss:  9.77e+01 Validation-Loss:  1.14e+02\n",
      "[Epoch    4/   5] [Batch   11/1326] Loss:  6.33e+01 Validation-Loss:  7.08e+01\n",
      "[Epoch    4/   5] [Batch   21/1326] Loss:  1.05e+02 Validation-Loss:  6.57e+01\n",
      "[Epoch    4/   5] [Batch   31/1326] Loss:  3.96e+01 Validation-Loss:  1.09e+02\n",
      "[Epoch    4/   5] [Batch   41/1326] Loss:  8.73e+01 Validation-Loss:  3.35e+01\n",
      "[Epoch    4/   5] [Batch   51/1326] Loss:  8.86e+01 Validation-Loss:  7.33e+01\n",
      "[Epoch    4/   5] [Batch   61/1326] Loss:  7.16e+01 Validation-Loss:  2.94e+01\n",
      "[Epoch    4/   5] [Batch   71/1326] Loss:  1.07e+02 Validation-Loss:  1.14e+02\n",
      "[Epoch    4/   5] [Batch   81/1326] Loss:  8.02e+01 Validation-Loss:  9.73e+01\n",
      "[Epoch    4/   5] [Batch   91/1326] Loss:  7.45e+01 Validation-Loss:  8.05e+01\n",
      "[Epoch    4/   5] [Batch  101/1326] Loss:  7.62e+01 Validation-Loss:  4.78e+01\n",
      "[Epoch    4/   5] [Batch  111/1326] Loss:  5.60e+01 Validation-Loss:  5.90e+01\n",
      "[Epoch    4/   5] [Batch  121/1326] Loss:  9.52e+01 Validation-Loss:  6.01e+01\n",
      "[Epoch    4/   5] [Batch  131/1326] Loss:  7.57e+01 Validation-Loss:  7.87e+01\n",
      "[Epoch    4/   5] [Batch  141/1326] Loss:  6.96e+01 Validation-Loss:  3.75e+01\n",
      "[Epoch    4/   5] [Batch  151/1326] Loss:  6.10e+01 Validation-Loss:  8.17e+01\n",
      "[Epoch    4/   5] [Batch  161/1326] Loss:  7.52e+01 Validation-Loss:  7.36e+01\n",
      "[Epoch    4/   5] [Batch  171/1326] Loss:  9.03e+01 Validation-Loss:  7.58e+01\n",
      "[Epoch    4/   5] [Batch  181/1326] Loss:  7.25e+01 Validation-Loss:  8.78e+01\n",
      "[Epoch    4/   5] [Batch  191/1326] Loss:  7.63e+01 Validation-Loss:  8.38e+01\n",
      "[Epoch    4/   5] [Batch  201/1326] Loss:  1.20e+02 Validation-Loss:  6.20e+01\n",
      "[Epoch    4/   5] [Batch  211/1326] Loss:  9.46e+01 Validation-Loss:  6.75e+01\n",
      "[Epoch    4/   5] [Batch  221/1326] Loss:  5.97e+01 Validation-Loss:  8.49e+01\n",
      "[Epoch    4/   5] [Batch  231/1326] Loss:  9.15e+01 Validation-Loss:  8.50e+01\n",
      "[Epoch    4/   5] [Batch  241/1326] Loss:  4.76e+01 Validation-Loss:  8.88e+01\n",
      "[Epoch    4/   5] [Batch  251/1326] Loss:  9.01e+01 Validation-Loss:  8.51e+01\n",
      "[Epoch    4/   5] [Batch  261/1326] Loss:  6.96e+01 Validation-Loss:  8.83e+01\n",
      "[Epoch    4/   5] [Batch  271/1326] Loss:  6.37e+01 Validation-Loss:  7.79e+01\n",
      "[Epoch    4/   5] [Batch  281/1326] Loss:  7.36e+01 Validation-Loss:  7.77e+01\n",
      "[Epoch    4/   5] [Batch  291/1326] Loss:  1.11e+02 Validation-Loss:  5.83e+01\n",
      "[Epoch    4/   5] [Batch  301/1326] Loss:  6.05e+01 Validation-Loss:  5.69e+01\n",
      "[Epoch    4/   5] [Batch  311/1326] Loss:  8.69e+01 Validation-Loss:  3.63e+01\n",
      "[Epoch    4/   5] [Batch  321/1326] Loss:  8.09e+01 Validation-Loss:  9.51e+01\n",
      "[Epoch    4/   5] [Batch  331/1326] Loss:  1.01e+02 Validation-Loss:  9.52e+01\n",
      "[Epoch    4/   5] [Batch  341/1326] Loss:  4.05e+01 Validation-Loss:  4.78e+01\n",
      "[Epoch    4/   5] [Batch  351/1326] Loss:  1.04e+02 Validation-Loss:  1.00e+02\n",
      "[Epoch    4/   5] [Batch  361/1326] Loss:  9.69e+01 Validation-Loss:  8.67e+01\n",
      "[Epoch    4/   5] [Batch  371/1326] Loss:  1.41e+02 Validation-Loss:  6.19e+01\n",
      "[Epoch    4/   5] [Batch  381/1326] Loss:  7.79e+01 Validation-Loss:  1.11e+02\n",
      "[Epoch    4/   5] [Batch  391/1326] Loss:  7.17e+01 Validation-Loss:  1.28e+02\n",
      "[Epoch    4/   5] [Batch  401/1326] Loss:  1.68e+02 Validation-Loss:  6.14e+01\n",
      "[Epoch    4/   5] [Batch  411/1326] Loss:  6.72e+01 Validation-Loss:  6.89e+01\n",
      "[Epoch    4/   5] [Batch  421/1326] Loss:  8.88e+01 Validation-Loss:  8.15e+01\n",
      "[Epoch    4/   5] [Batch  431/1326] Loss:  8.94e+01 Validation-Loss:  8.06e+01\n",
      "[Epoch    4/   5] [Batch  441/1326] Loss:  9.57e+01 Validation-Loss:  7.03e+01\n",
      "[Epoch    4/   5] [Batch  451/1326] Loss:  6.60e+01 Validation-Loss:  9.12e+01\n",
      "[Epoch    4/   5] [Batch  461/1326] Loss:  7.52e+01 Validation-Loss:  1.31e+02\n",
      "[Epoch    4/   5] [Batch  471/1326] Loss:  5.58e+01 Validation-Loss:  9.09e+01\n",
      "[Epoch    4/   5] [Batch  481/1326] Loss:  8.52e+01 Validation-Loss:  9.30e+01\n",
      "[Epoch    4/   5] [Batch  491/1326] Loss:  6.37e+01 Validation-Loss:  7.27e+01\n",
      "[Epoch    4/   5] [Batch  501/1326] Loss:  6.02e+01 Validation-Loss:  4.19e+01\n",
      "[Epoch    4/   5] [Batch  511/1326] Loss:  7.60e+01 Validation-Loss:  9.02e+01\n",
      "[Epoch    4/   5] [Batch  521/1326] Loss:  1.21e+02 Validation-Loss:  9.47e+01\n",
      "[Epoch    4/   5] [Batch  531/1326] Loss:  1.95e+02 Validation-Loss:  6.19e+01\n",
      "[Epoch    4/   5] [Batch  541/1326] Loss:  6.08e+01 Validation-Loss:  1.07e+02\n",
      "[Epoch    4/   5] [Batch  551/1326] Loss:  4.46e+01 Validation-Loss:  1.02e+02\n",
      "[Epoch    4/   5] [Batch  561/1326] Loss:  1.04e+02 Validation-Loss:  9.82e+01\n",
      "[Epoch    4/   5] [Batch  571/1326] Loss:  6.68e+01 Validation-Loss:  7.02e+01\n",
      "[Epoch    4/   5] [Batch  581/1326] Loss:  4.13e+01 Validation-Loss:  1.14e+02\n",
      "[Epoch    4/   5] [Batch  591/1326] Loss:  7.17e+01 Validation-Loss:  1.01e+02\n",
      "[Epoch    4/   5] [Batch  601/1326] Loss:  6.87e+01 Validation-Loss:  4.69e+01\n",
      "[Epoch    4/   5] [Batch  611/1326] Loss:  8.77e+01 Validation-Loss:  1.06e+02\n",
      "[Epoch    4/   5] [Batch  621/1326] Loss:  8.22e+01 Validation-Loss:  1.00e+02\n",
      "[Epoch    4/   5] [Batch  631/1326] Loss:  5.65e+01 Validation-Loss:  6.55e+01\n",
      "[Epoch    4/   5] [Batch  641/1326] Loss:  1.08e+02 Validation-Loss:  8.66e+01\n",
      "[Epoch    4/   5] [Batch  651/1326] Loss:  6.87e+01 Validation-Loss:  6.67e+01\n",
      "[Epoch    4/   5] [Batch  661/1326] Loss:  9.48e+01 Validation-Loss:  9.77e+01\n",
      "[Epoch    4/   5] [Batch  671/1326] Loss:  8.58e+01 Validation-Loss:  7.35e+01\n",
      "[Epoch    4/   5] [Batch  681/1326] Loss:  4.34e+01 Validation-Loss:  8.78e+01\n",
      "[Epoch    4/   5] [Batch  691/1326] Loss:  6.25e+01 Validation-Loss:  3.95e+01\n",
      "[Epoch    4/   5] [Batch  701/1326] Loss:  5.13e+01 Validation-Loss:  1.06e+02\n",
      "[Epoch    4/   5] [Batch  711/1326] Loss:  8.40e+01 Validation-Loss:  8.08e+01\n",
      "[Epoch    4/   5] [Batch  721/1326] Loss:  7.77e+01 Validation-Loss:  7.84e+01\n",
      "[Epoch    4/   5] [Batch  731/1326] Loss:  8.27e+01 Validation-Loss:  1.34e+02\n",
      "[Epoch    4/   5] [Batch  741/1326] Loss:  9.01e+01 Validation-Loss:  8.27e+01\n",
      "[Epoch    4/   5] [Batch  751/1326] Loss:  9.27e+01 Validation-Loss:  1.01e+02\n",
      "[Epoch    4/   5] [Batch  761/1326] Loss:  7.26e+01 Validation-Loss:  6.32e+01\n",
      "[Epoch    4/   5] [Batch  771/1326] Loss:  1.09e+02 Validation-Loss:  8.26e+01\n",
      "[Epoch    4/   5] [Batch  781/1326] Loss:  9.45e+01 Validation-Loss:  8.77e+01\n",
      "[Epoch    4/   5] [Batch  791/1326] Loss:  8.59e+01 Validation-Loss:  8.21e+01\n",
      "[Epoch    4/   5] [Batch  801/1326] Loss:  9.74e+01 Validation-Loss:  4.62e+01\n",
      "[Epoch    4/   5] [Batch  811/1326] Loss:  7.73e+01 Validation-Loss:  7.50e+01\n",
      "[Epoch    4/   5] [Batch  821/1326] Loss:  8.10e+01 Validation-Loss:  3.53e+01\n",
      "[Epoch    4/   5] [Batch  831/1326] Loss:  1.08e+02 Validation-Loss:  8.05e+01\n",
      "[Epoch    4/   5] [Batch  841/1326] Loss:  8.54e+01 Validation-Loss:  9.06e+01\n",
      "[Epoch    4/   5] [Batch  851/1326] Loss:  8.37e+01 Validation-Loss:  1.05e+02\n",
      "[Epoch    4/   5] [Batch  861/1326] Loss:  7.84e+01 Validation-Loss:  5.22e+01\n",
      "[Epoch    4/   5] [Batch  871/1326] Loss:  9.48e+01 Validation-Loss:  4.49e+01\n",
      "[Epoch    4/   5] [Batch  881/1326] Loss:  4.05e+01 Validation-Loss:  6.15e+01\n",
      "[Epoch    4/   5] [Batch  891/1326] Loss:  7.41e+01 Validation-Loss:  8.22e+01\n",
      "[Epoch    4/   5] [Batch  901/1326] Loss:  7.95e+01 Validation-Loss:  5.55e+01\n",
      "[Epoch    4/   5] [Batch  911/1326] Loss:  1.07e+02 Validation-Loss:  8.73e+01\n",
      "[Epoch    4/   5] [Batch  921/1326] Loss:  1.13e+02 Validation-Loss:  9.39e+01\n",
      "[Epoch    4/   5] [Batch  931/1326] Loss:  6.66e+01 Validation-Loss:  6.55e+01\n",
      "[Epoch    4/   5] [Batch  941/1326] Loss:  1.06e+02 Validation-Loss:  9.07e+01\n",
      "[Epoch    4/   5] [Batch  951/1326] Loss:  7.51e+01 Validation-Loss:  4.92e+01\n",
      "[Epoch    4/   5] [Batch  961/1326] Loss:  5.45e+01 Validation-Loss:  1.43e+02\n",
      "[Epoch    4/   5] [Batch  971/1326] Loss:  5.12e+01 Validation-Loss:  5.27e+01\n",
      "[Epoch    4/   5] [Batch  981/1326] Loss:  6.43e+01 Validation-Loss:  1.30e+02\n",
      "[Epoch    4/   5] [Batch  991/1326] Loss:  3.88e+01 Validation-Loss:  7.95e+01\n",
      "[Epoch    4/   5] [Batch 1001/1326] Loss:  5.69e+01 Validation-Loss:  7.49e+01\n",
      "[Epoch    4/   5] [Batch 1011/1326] Loss:  7.16e+01 Validation-Loss:  1.43e+02\n",
      "[Epoch    4/   5] [Batch 1021/1326] Loss:  9.95e+01 Validation-Loss:  8.39e+01\n",
      "[Epoch    4/   5] [Batch 1031/1326] Loss:  8.26e+01 Validation-Loss:  7.45e+01\n",
      "[Epoch    4/   5] [Batch 1041/1326] Loss:  9.75e+01 Validation-Loss:  1.00e+02\n",
      "[Epoch    4/   5] [Batch 1051/1326] Loss:  1.05e+02 Validation-Loss:  6.56e+01\n",
      "[Epoch    4/   5] [Batch 1061/1326] Loss:  7.21e+01 Validation-Loss:  6.42e+01\n",
      "[Epoch    4/   5] [Batch 1071/1326] Loss:  9.62e+01 Validation-Loss:  9.35e+01\n",
      "[Epoch    4/   5] [Batch 1081/1326] Loss:  1.08e+02 Validation-Loss:  8.64e+01\n",
      "[Epoch    4/   5] [Batch 1091/1326] Loss:  9.19e+01 Validation-Loss:  9.42e+01\n",
      "[Epoch    4/   5] [Batch 1101/1326] Loss:  6.57e+01 Validation-Loss:  1.02e+02\n",
      "[Epoch    4/   5] [Batch 1111/1326] Loss:  8.63e+01 Validation-Loss:  1.30e+02\n",
      "[Epoch    4/   5] [Batch 1121/1326] Loss:  1.04e+02 Validation-Loss:  3.85e+01\n",
      "[Epoch    4/   5] [Batch 1131/1326] Loss:  8.62e+01 Validation-Loss:  9.88e+01\n",
      "[Epoch    4/   5] [Batch 1141/1326] Loss:  7.27e+01 Validation-Loss:  6.47e+01\n",
      "[Epoch    4/   5] [Batch 1151/1326] Loss:  1.01e+02 Validation-Loss:  8.15e+01\n",
      "[Epoch    4/   5] [Batch 1161/1326] Loss:  5.31e+01 Validation-Loss:  6.08e+01\n",
      "[Epoch    4/   5] [Batch 1171/1326] Loss:  7.56e+01 Validation-Loss:  6.80e+01\n",
      "[Epoch    4/   5] [Batch 1181/1326] Loss:  6.50e+01 Validation-Loss:  7.50e+01\n",
      "[Epoch    4/   5] [Batch 1191/1326] Loss:  7.32e+01 Validation-Loss:  8.15e+01\n",
      "[Epoch    4/   5] [Batch 1201/1326] Loss:  6.24e+01 Validation-Loss:  9.37e+01\n",
      "[Epoch    4/   5] [Batch 1211/1326] Loss:  1.09e+02 Validation-Loss:  9.40e+01\n",
      "[Epoch    4/   5] [Batch 1221/1326] Loss:  4.74e+01 Validation-Loss:  9.97e+01\n",
      "[Epoch    4/   5] [Batch 1231/1326] Loss:  8.05e+01 Validation-Loss:  8.80e+01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/RNARepLearn/train.py:181\u001b[0m, in \u001b[0;36mTETraining.run\u001b[0;34m(self, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    179\u001b[0m step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    183\u001b[0m         batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/RNARepLearn/datasets.py:144\u001b[0m, in \u001b[0;36mDataset_UTR5.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 144\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_geometric\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mData(x\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m\"\u001b[39m],edge_index\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medges\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mt()\u001b[38;5;241m.\u001b[39mcontiguous(),edge_weight\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m],mrl\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmrl\u001b[39m\u001b[38;5;124m\"\u001b[39m],ID\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m    995\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_UntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_TypedStorage(\n\u001b[1;32m   1001\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1002\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training.run(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd77e3a8-a627-4d36-89ba-76c806cc40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5d87fed-38df-42b2-8917-bc6db1253087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54020/3277943181.py:1: DtypeWarning: Columns (33,34,35,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv = pd.read_csv(\"/lustre/groups/crna01/workspace/nicolas_msc/RNARepLearn/5utr/5UTR_design/raw/GSM3130443_designed_library.csv\")\n"
     ]
    }
   ],
   "source": [
    "csv = pd.read_csv(\"/lustre/groups/crna01/workspace/nicolas_msc/RNARepLearn/5utr/5UTR_design/raw/GSM3130443_designed_library.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f47446a6-9e78-4ecd-8432-7cbf8227dd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53029"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv['id'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "354ae126-0980-498d-a5ff-7eb04bafb7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>utr</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>rl</th>\n",
       "      <th>id</th>\n",
       "      <th>info1</th>\n",
       "      <th>info2</th>\n",
       "      <th>info3</th>\n",
       "      <th>info4</th>\n",
       "      <th>library</th>\n",
       "      <th>mother</th>\n",
       "      <th>designed</th>\n",
       "      <th>match_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>CCCACCCCGGGCTCTCTCCTGGCCTCCCACCCCCGCGCCCGGCTTC...</td>\n",
       "      <td>1763</td>\n",
       "      <td>1880</td>\n",
       "      <td>2154</td>\n",
       "      <td>2959</td>\n",
       "      <td>2122</td>\n",
       "      <td>1896</td>\n",
       "      <td>2618</td>\n",
       "      <td>1899</td>\n",
       "      <td>...</td>\n",
       "      <td>4.823082</td>\n",
       "      <td>NC_000012.12:g.4911352C&gt;T</td>\n",
       "      <td>rs886049508</td>\n",
       "      <td>chr12:4911298-4911381</td>\n",
       "      <td>snv</td>\n",
       "      <td>normal</td>\n",
       "      <td>snv</td>\n",
       "      <td>CCCACCCCGGGCTCTCTCCTGGCCTCCCACCCCCGCGCCCGGCTTC...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CCGTTCCTCCCCGCAGTCCTTCCCCTCCACTCCCTTCCCCTTCTCT...</td>\n",
       "      <td>1108</td>\n",
       "      <td>1317</td>\n",
       "      <td>774</td>\n",
       "      <td>1201</td>\n",
       "      <td>1813</td>\n",
       "      <td>2011</td>\n",
       "      <td>3340</td>\n",
       "      <td>2454</td>\n",
       "      <td>...</td>\n",
       "      <td>6.252208</td>\n",
       "      <td>8527</td>\n",
       "      <td>ENSG00000105492</td>\n",
       "      <td>ENST00000343300</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>human_utrs</td>\n",
       "      <td>CCGTTCCTCCCCGCAGTCCTTCCCCTCCACTCCCTTCCCCTTCTCT...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CCCTGATAACTGAATTGTTGTTTGTTTTATTTGTATTGTTGTTTGT...</td>\n",
       "      <td>918</td>\n",
       "      <td>894</td>\n",
       "      <td>523</td>\n",
       "      <td>782</td>\n",
       "      <td>877</td>\n",
       "      <td>999</td>\n",
       "      <td>1436</td>\n",
       "      <td>1799</td>\n",
       "      <td>...</td>\n",
       "      <td>7.499574</td>\n",
       "      <td>3409</td>\n",
       "      <td>20</td>\n",
       "      <td>8.81778335571289</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>target_no_uaug_allow_stop</td>\n",
       "      <td>CCCTGATAACTGAATTGTTGTTTGTTTTATTTGTATTGTTGTTTGT...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CCTCTGCCCGCCGTTCTGCTCGCTCGCTCCCCGCTCTGGAGTCTGC...</td>\n",
       "      <td>1439</td>\n",
       "      <td>1468</td>\n",
       "      <td>1322</td>\n",
       "      <td>1890</td>\n",
       "      <td>1347</td>\n",
       "      <td>1287</td>\n",
       "      <td>2007</td>\n",
       "      <td>2121</td>\n",
       "      <td>...</td>\n",
       "      <td>5.835379</td>\n",
       "      <td>8772</td>\n",
       "      <td>ENSG00000124570</td>\n",
       "      <td>ENST00000335686</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>human_utrs</td>\n",
       "      <td>CCTCTGCCCGCCGTTCTGCTCGCTCGCTCCCCGCTCTGGAGTCTGC...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>CCGGCCCCGAGGGCAGGCTCTCCCCGGAGGCTCAGCCCCCTCTGCT...</td>\n",
       "      <td>952</td>\n",
       "      <td>876</td>\n",
       "      <td>854</td>\n",
       "      <td>1675</td>\n",
       "      <td>2170</td>\n",
       "      <td>1825</td>\n",
       "      <td>2585</td>\n",
       "      <td>1385</td>\n",
       "      <td>...</td>\n",
       "      <td>5.433105</td>\n",
       "      <td>6322</td>\n",
       "      <td>ENSG00000149932</td>\n",
       "      <td>ENST00000279396</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>human_utrs</td>\n",
       "      <td>CCGGCCCCGAGGGCAGGCTCTCCCCGGAGGCTCAGCCCCCTCTGCT...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100012</th>\n",
       "      <td>53024</td>\n",
       "      <td>CCGAGAGCAGGAGGGCGCATCCGTGGCCGTTCACATGGTTTCAAGG...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>4.089237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>human_utrs</td>\n",
       "      <td>CCGAGAGCAGGCGGGCGCATCCGTGGCCGTTCACATGGTTTCAAGG...</td>\n",
       "      <td>False</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100013</th>\n",
       "      <td>53025</td>\n",
       "      <td>CAATTTGTTGGTTTAGTCTTCTATCTGTCACCGACAGGAAAATCAT...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>7.380217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>target_no_uaug_allow_stop</td>\n",
       "      <td>AAATTTGTTGGTTTAGTCTTCTATCTGTCACCGACAGGAAAATCAT...</td>\n",
       "      <td>False</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100014</th>\n",
       "      <td>53026</td>\n",
       "      <td>CCGGGATACTTCACGTTAGGGGTCGGAAAAGATGAAACACAGGAAG...</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.335412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>egfp_controls</td>\n",
       "      <td>CCGGGCTACTTCACGTTAGGGGTCGGAAAAGATGAAACACAGGAAG...</td>\n",
       "      <td>False</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100015</th>\n",
       "      <td>53027</td>\n",
       "      <td>CATTCTATTTTGGAATCTCTATGTGTTTGGTTCAAATGGCGAAAAA...</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>7.121724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>step_worst_to_best_allow_uatg</td>\n",
       "      <td>TATTCTATTTTGGAATCTCTATGTGTTTGGTTCAAATGGCGAAAAA...</td>\n",
       "      <td>False</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100016</th>\n",
       "      <td>53028</td>\n",
       "      <td>CGACCCGACCGCACCCGGCGCCTGCCCTCGCTCGGCGTCCCCGGTC...</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5.694511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>snv</td>\n",
       "      <td>CGACCCGACCGCACCCGGCGCCTGCCCTCGCTCGGCGTCCCCGGTC...</td>\n",
       "      <td>False</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100017 rows  42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                                utr     0  \\\n",
       "0                0  CCCACCCCGGGCTCTCTCCTGGCCTCCCACCCCCGCGCCCGGCTTC...  1763   \n",
       "1                2  CCGTTCCTCCCCGCAGTCCTTCCCCTCCACTCCCTTCCCCTTCTCT...  1108   \n",
       "2                3  CCCTGATAACTGAATTGTTGTTTGTTTTATTTGTATTGTTGTTTGT...   918   \n",
       "3                4  CCTCTGCCCGCCGTTCTGCTCGCTCGCTCCCCGCTCTGGAGTCTGC...  1439   \n",
       "4                5  CCGGCCCCGAGGGCAGGCTCTCCCCGGAGGCTCAGCCCCCTCTGCT...   952   \n",
       "...            ...                                                ...   ...   \n",
       "100012       53024  CCGAGAGCAGGAGGGCGCATCCGTGGCCGTTCACATGGTTTCAAGG...     6   \n",
       "100013       53025  CAATTTGTTGGTTTAGTCTTCTATCTGTCACCGACAGGAAAATCAT...     6   \n",
       "100014       53026  CCGGGATACTTCACGTTAGGGGTCGGAAAAGATGAAACACAGGAAG...     6   \n",
       "100015       53027  CATTCTATTTTGGAATCTCTATGTGTTTGGTTCAAATGGCGAAAAA...    15   \n",
       "100016       53028  CGACCCGACCGCACCCGGCGCCTGCCCTCGCTCGGCGTCCCCGGTC...    11   \n",
       "\n",
       "           1     2     3     4     5     6     7  ...        rl  \\\n",
       "0       1880  2154  2959  2122  1896  2618  1899  ...  4.823082   \n",
       "1       1317   774  1201  1813  2011  3340  2454  ...  6.252208   \n",
       "2        894   523   782   877   999  1436  1799  ...  7.499574   \n",
       "3       1468  1322  1890  1347  1287  2007  2121  ...  5.835379   \n",
       "4        876   854  1675  2170  1825  2585  1385  ...  5.433105   \n",
       "...      ...   ...   ...   ...   ...   ...   ...  ...       ...   \n",
       "100012     5     5    15    22    17    13     6  ...  4.089237   \n",
       "100013     7     6     3     4     3     8     8  ...  7.380217   \n",
       "100014     9    16    28    16    12     7     0  ...  2.335412   \n",
       "100015     3     0     4     0     6     3    10  ...  7.121724   \n",
       "100016     7     5    11     3     9    15     5  ...  5.694511   \n",
       "\n",
       "                               id            info1                  info2  \\\n",
       "0       NC_000012.12:g.4911352C>T      rs886049508  chr12:4911298-4911381   \n",
       "1                            8527  ENSG00000105492        ENST00000343300   \n",
       "2                            3409               20       8.81778335571289   \n",
       "3                            8772  ENSG00000124570        ENST00000335686   \n",
       "4                            6322  ENSG00000149932        ENST00000279396   \n",
       "...                           ...              ...                    ...   \n",
       "100012                        NaN              NaN                    NaN   \n",
       "100013                        NaN              NaN                    NaN   \n",
       "100014                        NaN              NaN                    NaN   \n",
       "100015                        NaN              NaN                    NaN   \n",
       "100016                        NaN              NaN                    NaN   \n",
       "\n",
       "        info3   info4                        library  \\\n",
       "0         snv  normal                            snv   \n",
       "1           -       -                     human_utrs   \n",
       "2           -       -      target_no_uaug_allow_stop   \n",
       "3           -       -                     human_utrs   \n",
       "4           -       -                     human_utrs   \n",
       "...       ...     ...                            ...   \n",
       "100012    NaN     NaN                     human_utrs   \n",
       "100013    NaN     NaN      target_no_uaug_allow_stop   \n",
       "100014    NaN     NaN                  egfp_controls   \n",
       "100015    NaN     NaN  step_worst_to_best_allow_uatg   \n",
       "100016    NaN     NaN                            snv   \n",
       "\n",
       "                                                   mother  designed  \\\n",
       "0       CCCACCCCGGGCTCTCTCCTGGCCTCCCACCCCCGCGCCCGGCTTC...      True   \n",
       "1       CCGTTCCTCCCCGCAGTCCTTCCCCTCCACTCCCTTCCCCTTCTCT...      True   \n",
       "2       CCCTGATAACTGAATTGTTGTTTGTTTTATTTGTATTGTTGTTTGT...      True   \n",
       "3       CCTCTGCCCGCCGTTCTGCTCGCTCGCTCCCCGCTCTGGAGTCTGC...      True   \n",
       "4       CCGGCCCCGAGGGCAGGCTCTCCCCGGAGGCTCAGCCCCCTCTGCT...      True   \n",
       "...                                                   ...       ...   \n",
       "100012  CCGAGAGCAGGCGGGCGCATCCGTGGCCGTTCACATGGTTTCAAGG...     False   \n",
       "100013  AAATTTGTTGGTTTAGTCTTCTATCTGTCACCGACAGGAAAATCAT...     False   \n",
       "100014  CCGGGCTACTTCACGTTAGGGGTCGGAAAAGATGAAACACAGGAAG...     False   \n",
       "100015  TATTCTATTTTGGAATCTCTATGTGTTTGGTTCAAATGGCGAAAAA...     False   \n",
       "100016  CGACCCGACCGCACCCGGCGCCTGCCCTCGCTCGGCGTCCCCGGTC...     False   \n",
       "\n",
       "        match_score  \n",
       "0               0.0  \n",
       "1               0.0  \n",
       "2               0.0  \n",
       "3               0.0  \n",
       "4               0.0  \n",
       "...             ...  \n",
       "100012         98.0  \n",
       "100013         96.0  \n",
       "100014         98.0  \n",
       "100015         98.0  \n",
       "100016         98.0  \n",
       "\n",
       "[100017 rows x 42 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d0a253a-3d14-438f-a4b6-688e7cdc1c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv['library'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d10d3e4-ad5d-4194-815b-461f3ad339b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RFAM",
   "language": "python",
   "name": "rfam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
