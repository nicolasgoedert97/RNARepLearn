
Due to MODULEPATH changes, the following have been reloaded:
  1) GCCcore/.11.3.0     3) StdEnv/2023        5) zlib/.1.2.12
  2) Stages/2023         4) binutils/.2.38

Avalaible threads: 24
DEBUG
Namespace(gin='/p/project/hai_rnareplearn/RNARepLearn/config/PreTrained_Models/pretrain_RPI.gin', output='/p/project/hai_rnareplearn/RUNS/600k_pretraining/runsRPI/fold0', dataset_path='/p/project/hai_rnareplearn/under_300', dataset_type='RFAM', test_on_train=False, dataset_names=None, write_datasets=False, train_indices='/p/project/hai_rnareplearn/RUNS/600k_pretraining/indices/folds/fold0/train.indices', val_indices='/p/project/hai_rnareplearn/RUNS/600k_pretraining/indices/folds/fold0/val.indices', test_indices=None, eval_model=None, train_split=None, val_split=None, test_split=None, data_parallel=False, train_mode='masked', model_state_dict=None, lr=None)
None
Dataset length: 695605
Training:	445188
Validation:	111296
Model: 
Encoder_Decoder_Model(
  (model): Sequential(
    (0): RPINetEncoder(
      (body): Sequential(
        (0): RPINetGNNLayer()
        (1): RPINetGNNLayer()
        (2): RPINetGNNLayer()
        (3): RPINetGNNLayer()
        (4): RPINetGNNLayer()
        (5): RPINetGNNLayer()
        (6): RPINetGNNLayer()
        (7): RPINetGNNLayer()
        (8): RPINetGNNLayer()
        (9): LSTM(256, 256, bidirectional=True)
      )
    )
    (1): AttentionDecoder(
      (key_projection): Linear(512, 512, bias=True)
      (query_projection): Linear(512, 512, bias=True)
      (nuc_projection): Linear(512, 4, bias=True)
    )
  )
)
Training running on device: cuda
Traceback (most recent call last):
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/bin/rnareplearn", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/RNARepLearn/command_line.py", line 92, in main
    model, log_dir, train_device, train_mode = train(train_loader=train_loader, val_loader=val_loader, batch_size=batch_size, args=args, log_dir=args.output, data_parallel=args.data_parallel, model_state_dict=args.model_state_dict, mode=args.train_mode)
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/RNARepLearn/command_line.py", line 170, in train
    training.run(train_loader, val_loader)
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/RNARepLearn/train.py", line 88, in run
    loss.backward()
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.05 GiB (GPU 0; 39.39 GiB total capacity; 25.16 GiB already allocated; 4.64 GiB free; 33.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  In call to configurable 'train' (<function train at 0x14773a12dc60>)
