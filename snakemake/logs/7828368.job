
Due to MODULEPATH changes, the following have been reloaded:
  1) GCCcore/.11.3.0     3) StdEnv/2023        5) zlib/.1.2.12
  2) Stages/2023         4) binutils/.2.38

Avalaible threads: 24
DEBUG
Namespace(gin='/p/project/hai_rnareplearn/RNARepLearn/config/PreTrained_Models/pretrain_GCN.gin', output='/p/project/hai_rnareplearn/RUNS/600k_pretraining/runsGCN/fold0', dataset_path='/p/project/hai_rnareplearn/under_300', dataset_type='RFAM', test_on_train=False, dataset_names=None, write_datasets=False, train_indices='/p/project/hai_rnareplearn/RUNS/600k_pretraining/indices/folds/fold0/train.indices', val_indices='/p/project/hai_rnareplearn/RUNS/600k_pretraining/indices/folds/fold0/val.indices', test_indices=None, eval_model=None, train_split=None, val_split=None, test_split=None, data_parallel=False, train_mode='masked', model_state_dict=None, lr=None)
None
Dataset length: 695605
Training:	445188
Validation:	111296
Model: 
Encoder_Decoder_Model(
  (model): Sequential(
    (0): Seq_Struc_GNN(
      (body): Sequential(
        (0): Sep_Seq_Struc_Layer(
        (seq_op): CNN_Seq(
          (body): Conv1d(4, 512, kernel_size=(9,), stride=(1,), padding=same)
        )
        (struc_op): GCNConv(4, 512)
      )
        (1): <function relu at 0x14bda40ab920>
        (2): <function dropout at 0x14bda40ab4c0>
        (3): Sep_Seq_Struc_Layer(
        (seq_op): CNN_Seq(
          (body): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same)
        )
        (struc_op): GCNConv(512, 512)
      )
        (4): <function relu at 0x14bda40ab920>
        (5): <function dropout at 0x14bda40ab4c0>
        (6): Sep_Seq_Struc_Layer(
        (seq_op): CNN_Seq(
          (body): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same)
        )
        (struc_op): GCNConv(512, 512)
      )
        (7): <function relu at 0x14bda40ab920>
        (8): <function dropout at 0x14bda40ab4c0>
        (9): Sep_Seq_Struc_Layer(
        (seq_op): CNN_Seq(
          (body): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same)
        )
        (struc_op): GCNConv(512, 512)
      )
        (10): <function relu at 0x14bda40ab920>
        (11): <function dropout at 0x14bda40ab4c0>
        (12): Sep_Seq_Struc_Layer(
        (seq_op): CNN_Seq(
          (body): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same)
        )
        (struc_op): GCNConv(512, 512)
      )
        (13): <function relu at 0x14bda40ab920>
        (14): <function dropout at 0x14bda40ab4c0>
        (15): Sep_Seq_Struc_Layer(
        (seq_op): CNN_Seq(
          (body): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same)
        )
        (struc_op): GCNConv(512, 512)
      )
        (16): <function relu at 0x14bda40ab920>
        (17): <function dropout at 0x14bda40ab4c0>
        (18): Sep_Seq_Struc_Layer(
        (seq_op): CNN_Seq(
          (body): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same)
        )
        (struc_op): GCNConv(512, 512)
      )
        (19): <function relu at 0x14bda40ab920>
        (20): <function dropout at 0x14bda40ab4c0>
        (21): Sep_Seq_Struc_Layer(
        (seq_op): CNN_Seq(
          (body): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same)
        )
        (struc_op): GCNConv(512, 512)
      )
        (22): <function relu at 0x14bda40ab920>
        (23): <function dropout at 0x14bda40ab4c0>
        (24): Sep_Seq_Struc_Layer(
        (seq_op): CNN_Seq(
          (body): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same)
        )
        (struc_op): GCNConv(512, 512)
      )
        (25): <function relu at 0x14bda40ab920>
        (26): <function dropout at 0x14bda40ab4c0>
      )
    )
    (1): AttentionDecoder(
      (key_projection): Linear(512, 512, bias=True)
      (query_projection): Linear(512, 512, bias=True)
      (nuc_projection): Linear(512, 4, bias=True)
    )
  )
)
Training running on device: cuda
[Epoch    1/  10] [Batch   10/1739] Loss:  4.51e+04 Nucleotide-Loss:  1.40e+00 Edge-Loss:  4.51e+04  Memory  7131070976
Traceback (most recent call last):
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/bin/rnareplearn", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/RNARepLearn/command_line.py", line 92, in main
    model, log_dir, train_device, train_mode = train(train_loader=train_loader, val_loader=val_loader, batch_size=batch_size, args=args, log_dir=args.output, data_parallel=args.data_parallel, model_state_dict=args.model_state_dict, mode=args.train_mode)
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/RNARepLearn/command_line.py", line 170, in train
    training.run(train_loader, val_loader)
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/RNARepLearn/train.py", line 88, in run
    loss.backward()
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/p/project/hai_rnareplearn/miniconda3/envs/RL/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.56 GiB (GPU 0; 39.39 GiB total capacity; 19.58 GiB already allocated; 5.86 GiB free; 32.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  In call to configurable 'train' (<function train at 0x14bd55215c60>)
