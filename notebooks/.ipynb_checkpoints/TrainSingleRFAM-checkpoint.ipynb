{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b88dc5-1bb7-4763-a723-d127060a73eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/vnicolas.goedert/miniconda3/envs/RFAM/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch_geometric\n",
    "import pickle\n",
    "import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83abfb0e-9d96-4704-890b-919e224fe125",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfam_dir = \"../rfam/data/raw/processed/release-14.8\"\n",
    "rfam = \"RF04088\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e095ec9-6dca-45bc-88ef-b18ee157fbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SingleMaskedRfamDataset(4988)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from RNARepLearn.datasets import SingleMaskedRfamDataset\n",
    "rfam_dataset = SingleMaskedRfamDataset(rfam_dir,rfam,15)\n",
    "rfam_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "268fe005-5b4c-42eb-a1e6-69ea00416bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(rfam_dataset))\n",
    "test_size = len(rfam_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(rfam_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7be29e-fd0f-4ffd-b546-54777e8dea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3948b385-555c-45d0-97d8-70ccc3f35167",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Model\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(rfam_dataset.num_node_features, 64)\n",
    "        self.conv2 = GCNConv(64, 4)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953684ac-c581-4a18-b6f7-40de32d7528f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d6db26a-dc57-43d5-9b92-b6bbeed36b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "model = model.double()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "n_epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c0902e2-7468-4a06-bbd1-4ad1f1fc3f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch    1/  20] [Batch    1/ 125] Loss:  1.21e+00\n",
      "[Epoch    1/  20] [Batch    2/ 125] Loss:  1.16e+00\n",
      "[Epoch    1/  20] [Batch    3/ 125] Loss:  1.10e+00\n",
      "[Epoch    1/  20] [Batch    4/ 125] Loss:  1.05e+00\n",
      "[Epoch    1/  20] [Batch    5/ 125] Loss:  9.89e-01\n",
      "[Epoch    1/  20] [Batch    6/ 125] Loss:  9.44e-01\n",
      "[Epoch    1/  20] [Batch    7/ 125] Loss:  9.00e-01\n",
      "[Epoch    1/  20] [Batch    8/ 125] Loss:  8.59e-01\n",
      "[Epoch    1/  20] [Batch    9/ 125] Loss:  8.26e-01\n",
      "[Epoch    1/  20] [Batch   10/ 125] Loss:  7.87e-01\n",
      "[Epoch    1/  20] [Batch   11/ 125] Loss:  7.48e-01\n",
      "[Epoch    1/  20] [Batch   12/ 125] Loss:  7.24e-01\n",
      "[Epoch    1/  20] [Batch   13/ 125] Loss:  6.86e-01\n",
      "[Epoch    1/  20] [Batch   14/ 125] Loss:  6.68e-01\n",
      "[Epoch    1/  20] [Batch   15/ 125] Loss:  6.43e-01\n",
      "[Epoch    1/  20] [Batch   16/ 125] Loss:  6.10e-01\n",
      "[Epoch    1/  20] [Batch   17/ 125] Loss:  5.83e-01\n",
      "[Epoch    1/  20] [Batch   18/ 125] Loss:  5.73e-01\n",
      "[Epoch    1/  20] [Batch   19/ 125] Loss:  5.56e-01\n",
      "[Epoch    1/  20] [Batch   20/ 125] Loss:  5.41e-01\n",
      "[Epoch    1/  20] [Batch   21/ 125] Loss:  5.18e-01\n",
      "[Epoch    1/  20] [Batch   22/ 125] Loss:  5.06e-01\n",
      "[Epoch    1/  20] [Batch   23/ 125] Loss:  4.83e-01\n",
      "[Epoch    1/  20] [Batch   24/ 125] Loss:  4.78e-01\n",
      "[Epoch    1/  20] [Batch   25/ 125] Loss:  4.69e-01\n",
      "[Epoch    1/  20] [Batch   26/ 125] Loss:  4.62e-01\n",
      "[Epoch    1/  20] [Batch   27/ 125] Loss:  4.49e-01\n",
      "[Epoch    1/  20] [Batch   28/ 125] Loss:  4.44e-01\n",
      "[Epoch    1/  20] [Batch   29/ 125] Loss:  4.35e-01\n",
      "[Epoch    1/  20] [Batch   30/ 125] Loss:  4.25e-01\n",
      "[Epoch    1/  20] [Batch   31/ 125] Loss:  4.11e-01\n",
      "[Epoch    1/  20] [Batch   32/ 125] Loss:  4.05e-01\n",
      "[Epoch    1/  20] [Batch   33/ 125] Loss:  4.00e-01\n",
      "[Epoch    1/  20] [Batch   34/ 125] Loss:  3.96e-01\n",
      "[Epoch    1/  20] [Batch   35/ 125] Loss:  3.85e-01\n",
      "[Epoch    1/  20] [Batch   36/ 125] Loss:  3.79e-01\n",
      "[Epoch    1/  20] [Batch   37/ 125] Loss:  3.79e-01\n",
      "[Epoch    1/  20] [Batch   38/ 125] Loss:  3.64e-01\n",
      "[Epoch    1/  20] [Batch   39/ 125] Loss:  3.57e-01\n",
      "[Epoch    1/  20] [Batch   40/ 125] Loss:  3.53e-01\n",
      "[Epoch    1/  20] [Batch   41/ 125] Loss:  3.42e-01\n",
      "[Epoch    1/  20] [Batch   42/ 125] Loss:  3.35e-01\n",
      "[Epoch    1/  20] [Batch   43/ 125] Loss:  3.25e-01\n",
      "[Epoch    1/  20] [Batch   44/ 125] Loss:  3.21e-01\n",
      "[Epoch    1/  20] [Batch   45/ 125] Loss:  3.07e-01\n",
      "[Epoch    1/  20] [Batch   46/ 125] Loss:  3.08e-01\n",
      "[Epoch    1/  20] [Batch   47/ 125] Loss:  3.01e-01\n",
      "[Epoch    1/  20] [Batch   48/ 125] Loss:  2.92e-01\n",
      "[Epoch    1/  20] [Batch   49/ 125] Loss:  2.75e-01\n",
      "[Epoch    1/  20] [Batch   50/ 125] Loss:  2.72e-01\n",
      "[Epoch    1/  20] [Batch   51/ 125] Loss:  2.62e-01\n",
      "[Epoch    1/  20] [Batch   52/ 125] Loss:  2.52e-01\n",
      "[Epoch    1/  20] [Batch   53/ 125] Loss:  2.49e-01\n",
      "[Epoch    1/  20] [Batch   54/ 125] Loss:  2.35e-01\n",
      "[Epoch    1/  20] [Batch   55/ 125] Loss:  2.34e-01\n",
      "[Epoch    1/  20] [Batch   56/ 125] Loss:  2.22e-01\n",
      "[Epoch    1/  20] [Batch   57/ 125] Loss:  2.17e-01\n",
      "[Epoch    1/  20] [Batch   58/ 125] Loss:  2.06e-01\n",
      "[Epoch    1/  20] [Batch   59/ 125] Loss:  2.01e-01\n",
      "[Epoch    1/  20] [Batch   60/ 125] Loss:  1.90e-01\n",
      "[Epoch    1/  20] [Batch   61/ 125] Loss:  1.84e-01\n",
      "[Epoch    1/  20] [Batch   62/ 125] Loss:  1.71e-01\n",
      "[Epoch    1/  20] [Batch   63/ 125] Loss:  1.69e-01\n",
      "[Epoch    1/  20] [Batch   64/ 125] Loss:  1.63e-01\n",
      "[Epoch    1/  20] [Batch   65/ 125] Loss:  1.53e-01\n",
      "[Epoch    1/  20] [Batch   66/ 125] Loss:  1.48e-01\n",
      "[Epoch    1/  20] [Batch   67/ 125] Loss:  1.43e-01\n",
      "[Epoch    1/  20] [Batch   68/ 125] Loss:  1.41e-01\n",
      "[Epoch    1/  20] [Batch   69/ 125] Loss:  1.28e-01\n",
      "[Epoch    1/  20] [Batch   70/ 125] Loss:  1.25e-01\n",
      "[Epoch    1/  20] [Batch   71/ 125] Loss:  1.11e-01\n",
      "[Epoch    1/  20] [Batch   72/ 125] Loss:  1.16e-01\n",
      "[Epoch    1/  20] [Batch   73/ 125] Loss:  1.07e-01\n",
      "[Epoch    1/  20] [Batch   74/ 125] Loss:  9.81e-02\n",
      "[Epoch    1/  20] [Batch   75/ 125] Loss:  1.00e-01\n",
      "[Epoch    1/  20] [Batch   76/ 125] Loss:  9.16e-02\n",
      "[Epoch    1/  20] [Batch   77/ 125] Loss:  9.41e-02\n",
      "[Epoch    1/  20] [Batch   78/ 125] Loss:  8.77e-02\n",
      "[Epoch    1/  20] [Batch   79/ 125] Loss:  8.35e-02\n",
      "[Epoch    1/  20] [Batch   80/ 125] Loss:  7.50e-02\n",
      "[Epoch    1/  20] [Batch   81/ 125] Loss:  8.95e-02\n",
      "[Epoch    1/  20] [Batch   82/ 125] Loss:  7.86e-02\n",
      "[Epoch    1/  20] [Batch   83/ 125] Loss:  6.75e-02\n",
      "[Epoch    1/  20] [Batch   84/ 125] Loss:  7.35e-02\n",
      "[Epoch    1/  20] [Batch   85/ 125] Loss:  6.06e-02\n",
      "[Epoch    1/  20] [Batch   86/ 125] Loss:  6.12e-02\n",
      "[Epoch    1/  20] [Batch   87/ 125] Loss:  5.91e-02\n",
      "[Epoch    1/  20] [Batch   88/ 125] Loss:  5.56e-02\n",
      "[Epoch    1/  20] [Batch   89/ 125] Loss:  6.58e-02\n",
      "[Epoch    1/  20] [Batch   90/ 125] Loss:  5.00e-02\n",
      "[Epoch    1/  20] [Batch   91/ 125] Loss:  5.82e-02\n",
      "[Epoch    1/  20] [Batch   92/ 125] Loss:  5.15e-02\n",
      "[Epoch    1/  20] [Batch   93/ 125] Loss:  4.83e-02\n",
      "[Epoch    1/  20] [Batch   94/ 125] Loss:  5.15e-02\n",
      "[Epoch    1/  20] [Batch   95/ 125] Loss:  5.18e-02\n",
      "[Epoch    1/  20] [Batch   96/ 125] Loss:  4.92e-02\n",
      "[Epoch    1/  20] [Batch   97/ 125] Loss:  4.37e-02\n",
      "[Epoch    1/  20] [Batch   98/ 125] Loss:  4.45e-02\n",
      "[Epoch    1/  20] [Batch   99/ 125] Loss:  4.82e-02\n",
      "[Epoch    1/  20] [Batch  100/ 125] Loss:  4.42e-02\n",
      "[Epoch    1/  20] [Batch  101/ 125] Loss:  3.74e-02\n",
      "[Epoch    1/  20] [Batch  102/ 125] Loss:  3.80e-02\n",
      "[Epoch    1/  20] [Batch  103/ 125] Loss:  3.82e-02\n",
      "[Epoch    1/  20] [Batch  104/ 125] Loss:  3.98e-02\n",
      "[Epoch    1/  20] [Batch  105/ 125] Loss:  3.02e-02\n",
      "[Epoch    1/  20] [Batch  106/ 125] Loss:  3.37e-02\n",
      "[Epoch    1/  20] [Batch  107/ 125] Loss:  3.57e-02\n",
      "[Epoch    1/  20] [Batch  108/ 125] Loss:  3.31e-02\n",
      "[Epoch    1/  20] [Batch  109/ 125] Loss:  3.11e-02\n",
      "[Epoch    1/  20] [Batch  110/ 125] Loss:  2.82e-02\n",
      "[Epoch    1/  20] [Batch  111/ 125] Loss:  3.47e-02\n",
      "[Epoch    1/  20] [Batch  112/ 125] Loss:  2.74e-02\n",
      "[Epoch    1/  20] [Batch  113/ 125] Loss:  3.12e-02\n",
      "[Epoch    1/  20] [Batch  114/ 125] Loss:  3.19e-02\n",
      "[Epoch    1/  20] [Batch  115/ 125] Loss:  2.41e-02\n",
      "[Epoch    1/  20] [Batch  116/ 125] Loss:  2.40e-02\n",
      "[Epoch    1/  20] [Batch  117/ 125] Loss:  2.80e-02\n",
      "[Epoch    1/  20] [Batch  118/ 125] Loss:  2.69e-02\n",
      "[Epoch    1/  20] [Batch  119/ 125] Loss:  2.82e-02\n",
      "[Epoch    1/  20] [Batch  120/ 125] Loss:  2.62e-02\n",
      "[Epoch    1/  20] [Batch  121/ 125] Loss:  2.76e-02\n",
      "[Epoch    1/  20] [Batch  122/ 125] Loss:  2.30e-02\n",
      "[Epoch    1/  20] [Batch  123/ 125] Loss:  3.26e-02\n",
      "[Epoch    1/  20] [Batch  124/ 125] Loss:  3.56e-02\n",
      "[Epoch    1/  20] [Batch  125/ 125] Loss:  2.29e-02\n",
      "[Epoch    2/  20] [Batch    1/ 125] Loss:  2.55e-02\n",
      "[Epoch    2/  20] [Batch    2/ 125] Loss:  2.96e-02\n",
      "[Epoch    2/  20] [Batch    3/ 125] Loss:  2.89e-02\n",
      "[Epoch    2/  20] [Batch    4/ 125] Loss:  2.01e-02\n",
      "[Epoch    2/  20] [Batch    5/ 125] Loss:  2.18e-02\n",
      "[Epoch    2/  20] [Batch    6/ 125] Loss:  2.96e-02\n",
      "[Epoch    2/  20] [Batch    7/ 125] Loss:  2.18e-02\n",
      "[Epoch    2/  20] [Batch    8/ 125] Loss:  2.02e-02\n",
      "[Epoch    2/  20] [Batch    9/ 125] Loss:  2.07e-02\n",
      "[Epoch    2/  20] [Batch   10/ 125] Loss:  2.04e-02\n",
      "[Epoch    2/  20] [Batch   11/ 125] Loss:  2.00e-02\n",
      "[Epoch    2/  20] [Batch   12/ 125] Loss:  1.75e-02\n",
      "[Epoch    2/  20] [Batch   13/ 125] Loss:  1.99e-02\n",
      "[Epoch    2/  20] [Batch   14/ 125] Loss:  2.11e-02\n",
      "[Epoch    2/  20] [Batch   15/ 125] Loss:  2.14e-02\n",
      "[Epoch    2/  20] [Batch   16/ 125] Loss:  2.34e-02\n",
      "[Epoch    2/  20] [Batch   17/ 125] Loss:  2.19e-02\n",
      "[Epoch    2/  20] [Batch   18/ 125] Loss:  1.79e-02\n",
      "[Epoch    2/  20] [Batch   19/ 125] Loss:  2.17e-02\n",
      "[Epoch    2/  20] [Batch   20/ 125] Loss:  1.66e-02\n",
      "[Epoch    2/  20] [Batch   21/ 125] Loss:  1.90e-02\n",
      "[Epoch    2/  20] [Batch   22/ 125] Loss:  1.82e-02\n",
      "[Epoch    2/  20] [Batch   23/ 125] Loss:  1.99e-02\n",
      "[Epoch    2/  20] [Batch   24/ 125] Loss:  1.73e-02\n",
      "[Epoch    2/  20] [Batch   25/ 125] Loss:  2.13e-02\n",
      "[Epoch    2/  20] [Batch   26/ 125] Loss:  1.55e-02\n",
      "[Epoch    2/  20] [Batch   27/ 125] Loss:  1.77e-02\n",
      "[Epoch    2/  20] [Batch   28/ 125] Loss:  1.80e-02\n",
      "[Epoch    2/  20] [Batch   29/ 125] Loss:  2.17e-02\n",
      "[Epoch    2/  20] [Batch   30/ 125] Loss:  1.58e-02\n",
      "[Epoch    2/  20] [Batch   31/ 125] Loss:  2.23e-02\n",
      "[Epoch    2/  20] [Batch   32/ 125] Loss:  1.79e-02\n",
      "[Epoch    2/  20] [Batch   33/ 125] Loss:  1.48e-02\n",
      "[Epoch    2/  20] [Batch   34/ 125] Loss:  1.90e-02\n",
      "[Epoch    2/  20] [Batch   35/ 125] Loss:  1.69e-02\n",
      "[Epoch    2/  20] [Batch   36/ 125] Loss:  1.50e-02\n",
      "[Epoch    2/  20] [Batch   37/ 125] Loss:  1.44e-02\n",
      "[Epoch    2/  20] [Batch   38/ 125] Loss:  1.58e-02\n",
      "[Epoch    2/  20] [Batch   39/ 125] Loss:  2.02e-02\n",
      "[Epoch    2/  20] [Batch   40/ 125] Loss:  1.69e-02\n",
      "[Epoch    2/  20] [Batch   41/ 125] Loss:  1.50e-02\n",
      "[Epoch    2/  20] [Batch   42/ 125] Loss:  1.41e-02\n",
      "[Epoch    2/  20] [Batch   43/ 125] Loss:  1.51e-02\n",
      "[Epoch    2/  20] [Batch   44/ 125] Loss:  1.51e-02\n",
      "[Epoch    2/  20] [Batch   45/ 125] Loss:  1.75e-02\n",
      "[Epoch    2/  20] [Batch   46/ 125] Loss:  1.31e-02\n",
      "[Epoch    2/  20] [Batch   47/ 125] Loss:  1.22e-02\n",
      "[Epoch    2/  20] [Batch   48/ 125] Loss:  1.29e-02\n",
      "[Epoch    2/  20] [Batch   49/ 125] Loss:  1.21e-02\n",
      "[Epoch    2/  20] [Batch   50/ 125] Loss:  2.05e-02\n",
      "[Epoch    2/  20] [Batch   51/ 125] Loss:  1.53e-02\n",
      "[Epoch    2/  20] [Batch   52/ 125] Loss:  1.57e-02\n",
      "[Epoch    2/  20] [Batch   53/ 125] Loss:  1.36e-02\n",
      "[Epoch    2/  20] [Batch   54/ 125] Loss:  1.83e-02\n",
      "[Epoch    2/  20] [Batch   55/ 125] Loss:  1.82e-02\n",
      "[Epoch    2/  20] [Batch   56/ 125] Loss:  1.51e-02\n",
      "[Epoch    2/  20] [Batch   57/ 125] Loss:  1.40e-02\n",
      "[Epoch    2/  20] [Batch   58/ 125] Loss:  1.46e-02\n",
      "[Epoch    2/  20] [Batch   59/ 125] Loss:  1.71e-02\n",
      "[Epoch    2/  20] [Batch   60/ 125] Loss:  1.57e-02\n",
      "[Epoch    2/  20] [Batch   61/ 125] Loss:  1.41e-02\n",
      "[Epoch    2/  20] [Batch   62/ 125] Loss:  1.42e-02\n",
      "[Epoch    2/  20] [Batch   63/ 125] Loss:  1.46e-02\n",
      "[Epoch    2/  20] [Batch   64/ 125] Loss:  1.41e-02\n",
      "[Epoch    2/  20] [Batch   65/ 125] Loss:  1.54e-02\n",
      "[Epoch    2/  20] [Batch   66/ 125] Loss:  1.89e-02\n",
      "[Epoch    2/  20] [Batch   67/ 125] Loss:  1.31e-02\n",
      "[Epoch    2/  20] [Batch   68/ 125] Loss:  1.67e-02\n",
      "[Epoch    2/  20] [Batch   69/ 125] Loss:  1.45e-02\n",
      "[Epoch    2/  20] [Batch   70/ 125] Loss:  1.26e-02\n",
      "[Epoch    2/  20] [Batch   71/ 125] Loss:  1.73e-02\n",
      "[Epoch    2/  20] [Batch   72/ 125] Loss:  1.35e-02\n",
      "[Epoch    2/  20] [Batch   73/ 125] Loss:  1.23e-02\n",
      "[Epoch    2/  20] [Batch   74/ 125] Loss:  1.60e-02\n",
      "[Epoch    2/  20] [Batch   75/ 125] Loss:  1.38e-02\n",
      "[Epoch    2/  20] [Batch   76/ 125] Loss:  1.55e-02\n",
      "[Epoch    2/  20] [Batch   77/ 125] Loss:  1.34e-02\n",
      "[Epoch    2/  20] [Batch   78/ 125] Loss:  1.46e-02\n",
      "[Epoch    2/  20] [Batch   79/ 125] Loss:  1.24e-02\n",
      "[Epoch    2/  20] [Batch   80/ 125] Loss:  1.26e-02\n",
      "[Epoch    2/  20] [Batch   81/ 125] Loss:  1.58e-02\n",
      "[Epoch    2/  20] [Batch   82/ 125] Loss:  1.22e-02\n",
      "[Epoch    2/  20] [Batch   83/ 125] Loss:  1.16e-02\n",
      "[Epoch    2/  20] [Batch   84/ 125] Loss:  1.57e-02\n",
      "[Epoch    2/  20] [Batch   85/ 125] Loss:  1.81e-02\n",
      "[Epoch    2/  20] [Batch   86/ 125] Loss:  1.33e-02\n",
      "[Epoch    2/  20] [Batch   87/ 125] Loss:  1.54e-02\n",
      "[Epoch    2/  20] [Batch   88/ 125] Loss:  1.76e-02\n",
      "[Epoch    2/  20] [Batch   89/ 125] Loss:  1.43e-02\n",
      "[Epoch    2/  20] [Batch   90/ 125] Loss:  1.29e-02\n",
      "[Epoch    2/  20] [Batch   91/ 125] Loss:  1.72e-02\n",
      "[Epoch    2/  20] [Batch   92/ 125] Loss:  1.42e-02\n",
      "[Epoch    2/  20] [Batch   93/ 125] Loss:  1.12e-02\n",
      "[Epoch    2/  20] [Batch   94/ 125] Loss:  1.22e-02\n",
      "[Epoch    2/  20] [Batch   95/ 125] Loss:  1.27e-02\n",
      "[Epoch    2/  20] [Batch   96/ 125] Loss:  1.29e-02\n",
      "[Epoch    2/  20] [Batch   97/ 125] Loss:  1.25e-02\n",
      "[Epoch    2/  20] [Batch   98/ 125] Loss:  1.10e-02\n",
      "[Epoch    2/  20] [Batch   99/ 125] Loss:  1.96e-02\n",
      "[Epoch    2/  20] [Batch  100/ 125] Loss:  1.15e-02\n",
      "[Epoch    2/  20] [Batch  101/ 125] Loss:  1.05e-02\n",
      "[Epoch    2/  20] [Batch  102/ 125] Loss:  1.41e-02\n",
      "[Epoch    2/  20] [Batch  103/ 125] Loss:  9.66e-03\n",
      "[Epoch    2/  20] [Batch  104/ 125] Loss:  1.21e-02\n",
      "[Epoch    2/  20] [Batch  105/ 125] Loss:  1.19e-02\n",
      "[Epoch    2/  20] [Batch  106/ 125] Loss:  1.40e-02\n",
      "[Epoch    2/  20] [Batch  107/ 125] Loss:  1.45e-02\n",
      "[Epoch    2/  20] [Batch  108/ 125] Loss:  1.15e-02\n",
      "[Epoch    2/  20] [Batch  109/ 125] Loss:  1.44e-02\n",
      "[Epoch    2/  20] [Batch  110/ 125] Loss:  1.22e-02\n",
      "[Epoch    2/  20] [Batch  111/ 125] Loss:  1.26e-02\n",
      "[Epoch    2/  20] [Batch  112/ 125] Loss:  1.17e-02\n",
      "[Epoch    2/  20] [Batch  113/ 125] Loss:  1.10e-02\n",
      "[Epoch    2/  20] [Batch  114/ 125] Loss:  1.31e-02\n",
      "[Epoch    2/  20] [Batch  115/ 125] Loss:  1.34e-02\n",
      "[Epoch    2/  20] [Batch  116/ 125] Loss:  1.05e-02\n",
      "[Epoch    2/  20] [Batch  117/ 125] Loss:  1.65e-02\n",
      "[Epoch    2/  20] [Batch  118/ 125] Loss:  1.39e-02\n",
      "[Epoch    2/  20] [Batch  119/ 125] Loss:  1.02e-02\n",
      "[Epoch    2/  20] [Batch  120/ 125] Loss:  1.20e-02\n",
      "[Epoch    2/  20] [Batch  121/ 125] Loss:  1.11e-02\n",
      "[Epoch    2/  20] [Batch  122/ 125] Loss:  1.29e-02\n",
      "[Epoch    2/  20] [Batch  123/ 125] Loss:  1.32e-02\n",
      "[Epoch    2/  20] [Batch  124/ 125] Loss:  1.22e-02\n",
      "[Epoch    2/  20] [Batch  125/ 125] Loss:  1.22e-02\n",
      "[Epoch    3/  20] [Batch    1/ 125] Loss:  1.22e-02\n",
      "[Epoch    3/  20] [Batch    2/ 125] Loss:  1.23e-02\n",
      "[Epoch    3/  20] [Batch    3/ 125] Loss:  1.30e-02\n",
      "[Epoch    3/  20] [Batch    4/ 125] Loss:  1.38e-02\n",
      "[Epoch    3/  20] [Batch    5/ 125] Loss:  1.04e-02\n",
      "[Epoch    3/  20] [Batch    6/ 125] Loss:  1.45e-02\n",
      "[Epoch    3/  20] [Batch    7/ 125] Loss:  1.07e-02\n",
      "[Epoch    3/  20] [Batch    8/ 125] Loss:  1.27e-02\n",
      "[Epoch    3/  20] [Batch    9/ 125] Loss:  9.41e-03\n",
      "[Epoch    3/  20] [Batch   10/ 125] Loss:  1.20e-02\n",
      "[Epoch    3/  20] [Batch   11/ 125] Loss:  1.18e-02\n",
      "[Epoch    3/  20] [Batch   12/ 125] Loss:  1.68e-02\n",
      "[Epoch    3/  20] [Batch   13/ 125] Loss:  1.30e-02\n",
      "[Epoch    3/  20] [Batch   14/ 125] Loss:  1.01e-02\n",
      "[Epoch    3/  20] [Batch   15/ 125] Loss:  1.18e-02\n",
      "[Epoch    3/  20] [Batch   16/ 125] Loss:  1.11e-02\n",
      "[Epoch    3/  20] [Batch   17/ 125] Loss:  1.34e-02\n",
      "[Epoch    3/  20] [Batch   18/ 125] Loss:  1.11e-02\n",
      "[Epoch    3/  20] [Batch   19/ 125] Loss:  1.42e-02\n",
      "[Epoch    3/  20] [Batch   20/ 125] Loss:  1.17e-02\n",
      "[Epoch    3/  20] [Batch   21/ 125] Loss:  1.26e-02\n",
      "[Epoch    3/  20] [Batch   22/ 125] Loss:  1.12e-02\n",
      "[Epoch    3/  20] [Batch   23/ 125] Loss:  8.89e-03\n",
      "[Epoch    3/  20] [Batch   24/ 125] Loss:  1.05e-02\n",
      "[Epoch    3/  20] [Batch   25/ 125] Loss:  1.49e-02\n",
      "[Epoch    3/  20] [Batch   26/ 125] Loss:  1.22e-02\n",
      "[Epoch    3/  20] [Batch   27/ 125] Loss:  1.11e-02\n",
      "[Epoch    3/  20] [Batch   28/ 125] Loss:  1.05e-02\n",
      "[Epoch    3/  20] [Batch   29/ 125] Loss:  1.18e-02\n",
      "[Epoch    3/  20] [Batch   30/ 125] Loss:  1.28e-02\n",
      "[Epoch    3/  20] [Batch   31/ 125] Loss:  9.34e-03\n",
      "[Epoch    3/  20] [Batch   32/ 125] Loss:  1.22e-02\n",
      "[Epoch    3/  20] [Batch   33/ 125] Loss:  1.03e-02\n",
      "[Epoch    3/  20] [Batch   34/ 125] Loss:  1.09e-02\n",
      "[Epoch    3/  20] [Batch   35/ 125] Loss:  9.77e-03\n",
      "[Epoch    3/  20] [Batch   36/ 125] Loss:  1.12e-02\n",
      "[Epoch    3/  20] [Batch   37/ 125] Loss:  1.54e-02\n",
      "[Epoch    3/  20] [Batch   38/ 125] Loss:  1.20e-02\n",
      "[Epoch    3/  20] [Batch   39/ 125] Loss:  1.13e-02\n",
      "[Epoch    3/  20] [Batch   40/ 125] Loss:  1.09e-02\n",
      "[Epoch    3/  20] [Batch   41/ 125] Loss:  1.23e-02\n",
      "[Epoch    3/  20] [Batch   42/ 125] Loss:  1.05e-02\n",
      "[Epoch    3/  20] [Batch   43/ 125] Loss:  1.18e-02\n",
      "[Epoch    3/  20] [Batch   44/ 125] Loss:  8.50e-03\n",
      "[Epoch    3/  20] [Batch   45/ 125] Loss:  1.52e-02\n",
      "[Epoch    3/  20] [Batch   46/ 125] Loss:  1.17e-02\n",
      "[Epoch    3/  20] [Batch   47/ 125] Loss:  1.03e-02\n",
      "[Epoch    3/  20] [Batch   48/ 125] Loss:  1.19e-02\n",
      "[Epoch    3/  20] [Batch   49/ 125] Loss:  9.63e-03\n",
      "[Epoch    3/  20] [Batch   50/ 125] Loss:  1.48e-02\n",
      "[Epoch    3/  20] [Batch   51/ 125] Loss:  1.06e-02\n",
      "[Epoch    3/  20] [Batch   52/ 125] Loss:  1.09e-02\n",
      "[Epoch    3/  20] [Batch   53/ 125] Loss:  1.07e-02\n",
      "[Epoch    3/  20] [Batch   54/ 125] Loss:  1.25e-02\n",
      "[Epoch    3/  20] [Batch   55/ 125] Loss:  1.25e-02\n",
      "[Epoch    3/  20] [Batch   56/ 125] Loss:  1.06e-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m lossf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      9\u001b[0m         batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/lustre/groups/crna01/workspace/nicolas_msc/RNARepLearn/RNARepLearn/datasets.py:97\u001b[0m, in \u001b[0;36mSingleMaskedRfamDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_file_names\u001b[49m[index]\n\u001b[1;32m     98\u001b[0m     data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdir,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_file_names[index]))\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_geometric\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mData(x\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m],edge_index\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medges\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mt()\u001b[38;5;241m.\u001b[39mcontiguous(),y\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m\"\u001b[39m],rfam\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrfam\u001b[39m\u001b[38;5;124m\"\u001b[39m],ID\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/lustre/groups/crna01/workspace/nicolas_msc/RNARepLearn/RNARepLearn/datasets.py:26\u001b[0m, in \u001b[0;36mSingleMaskedRfamDataset.processed_file_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocessed_file_names\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdir,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m))): \n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdir\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##Training\n",
    "train_hist = {}\n",
    "train_hist[\"loss\"]=[]\n",
    "model.train()\n",
    "lossf = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "    \n",
    "        loss = lossf(out,batch.y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_hist[\"loss\"].append(loss.item())\n",
    "        \n",
    "        print('\\r[Epoch %4d/%4d] [Batch %4d/%4d] Loss: % 2.2e' % (epoch + 1, n_epochs, \n",
    "                                                                idx + 1, len(train_loader), \n",
    "                                                                loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46b265-d9fc-4f05-819b-d9983b1db811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RFAM",
   "language": "python",
   "name": "rfam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
