{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c857e57-8f50-491e-9979-2f478d86e6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/vnicolas.goedert/miniconda3/envs/RFAM/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde15794-e4d9-43d1-87dd-c6f47579d4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF00005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF00169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF00050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF00020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "19085\n",
      "238\n",
      "4534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "rfam_dir = \"../rfam/data/raw/processed/release-14.8\"\n",
    "rfams = [\"RF00001\",\"RF00005\",\"RF00169\",\"RF00050\", \"RF00020\"]\n",
    "\n",
    "from RNARepLearn.datasets import CombinedRfamDataset, SingleRfamDataset\n",
    "#dataset = CombinedRfamDataset(rfam_dir, rfams, \"Under300\", 15, 300)\n",
    "#dataset = CombinedRfamDataset(rfam_dir, [\"RF00001\",\"RF00005\"], \"RF00001_RF00005\")\n",
    "#dataset = SingleRfamDataset(rfam_dir,\"RF00001\")\n",
    "dataset = CombinedRfamDataset(rfam_dir, rfams, \"Under1000_RF00001_RF00005_RF00169_RF00050_RF00020\", 1000)\n",
    "\n",
    "from RNARepLearn.utils import train_val_test_loaders\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_test_loaders(dataset, 0.8, 0.01, 0.19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "115fcb34-bcbe-4705-90d6-1dbf5bf41c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'DS562887.1/11698675-11698792', 'seq': 'AAAAATAGCAAGAAACCCTGAACGCGCCCGATCTCGTCTGATCTCGGAAGCTAAGCAGGGTCCGGCCTGGTTAGTACTTGGATGGGAGACCGCCTGGGAATATCGGGTGCTGTAGGCT', 'seq_int': array([0, 0, 0, 0, 0, 3, 0, 2, 1, 0, 0, 2, 0, 0, 0, 1, 1, 1, 3, 2, 0, 0,\n",
      "       1, 2, 1, 2, 1, 1, 1, 2, 0, 3, 1, 3, 1, 2, 3, 1, 3, 2, 0, 3, 1, 3,\n",
      "       1, 2, 2, 0, 0, 2, 1, 3, 0, 0, 2, 1, 0, 2, 2, 2, 3, 1, 1, 2, 2, 1,\n",
      "       1, 3, 2, 2, 3, 3, 0, 2, 3, 0, 1, 3, 3, 2, 2, 0, 3, 2, 2, 2, 0, 2,\n",
      "       0, 1, 1, 2, 1, 1, 3, 2, 2, 2, 0, 0, 3, 0, 3, 1, 2, 2, 2, 3, 2, 1,\n",
      "       3, 2, 3, 0, 2, 2, 1, 3], dtype=int8), 'len': 118, 'bpp': array([0., 0., 0., ..., 0., 0., 0.]), 'rfam_id': 'RF00001'}\n",
      "{'id': 'DS233273.1/56842-56915', 'seq': 'GCCCGTGTGGATCAATCGGACCGCGCACTGGACTCACAACCCAGAGGTCGCCGGTTCGAATCCCGCGGCGGGCG', 'seq_int': array([2, 1, 1, 1, 2, 3, 2, 3, 2, 2, 0, 3, 1, 0, 0, 3, 1, 2, 2, 0, 1, 1,\n",
      "       2, 1, 2, 1, 0, 1, 3, 2, 2, 0, 1, 3, 1, 0, 1, 0, 0, 1, 1, 1, 0, 2,\n",
      "       0, 2, 2, 3, 1, 2, 1, 1, 2, 2, 3, 3, 1, 2, 0, 0, 3, 1, 1, 1, 2, 1,\n",
      "       2, 2, 1, 2, 2, 2, 1, 2], dtype=int8), 'len': 74, 'bpp': array([0., 0., 0., ..., 0., 0., 0.]), 'rfam_id': 'RF00005'}\n",
      "{'id': 'KI530762.1/1263899-1263995', 'seq': 'TGCTGGTAGCCTCGCAATTATACTCTGTGAACCCCGCCAGGACCGGAAGGTAGCAACGGTAGCAGATCTATGATGTGCCGAAGTTTCGCTAGTAGGG', 'seq_int': array([3, 2, 1, 3, 2, 2, 3, 0, 2, 1, 1, 3, 1, 2, 1, 0, 0, 3, 3, 0, 3, 0,\n",
      "       1, 3, 1, 3, 2, 3, 2, 0, 0, 1, 1, 1, 1, 2, 1, 1, 0, 2, 2, 0, 1, 1,\n",
      "       2, 2, 0, 0, 2, 2, 3, 0, 2, 1, 0, 0, 1, 2, 2, 3, 0, 2, 1, 0, 2, 0,\n",
      "       3, 1, 3, 0, 3, 2, 0, 3, 2, 3, 2, 1, 1, 2, 0, 0, 2, 3, 3, 3, 1, 2,\n",
      "       1, 3, 0, 2, 3, 0, 2, 2, 2], dtype=int8), 'len': 97, 'bpp': array([0., 0., 0., ..., 0., 0., 0.]), 'rfam_id': 'RF00169'}\n",
      "{'id': 'BA000032.2/1323634-1323774', 'seq': 'AATTGTTCTCAGGGCGGGGCGAAATTCCCCACCGGCGGTATACTTTTTCAAGTGAGCCCGCGAGCGCTCGATTCGTCGAGGTCAGCAGATCTGGTGAGATGCCAGAGCCGACGGTTATAGTCCGGATGAAAGAGAATAAGA', 'seq_int': array([0, 0, 3, 3, 2, 3, 3, 1, 3, 1, 0, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 0,\n",
      "       0, 0, 3, 3, 1, 1, 1, 1, 0, 1, 1, 2, 2, 1, 2, 2, 3, 0, 3, 0, 1, 3,\n",
      "       3, 3, 3, 3, 1, 0, 0, 2, 3, 2, 0, 2, 1, 1, 1, 2, 1, 2, 0, 2, 1, 2,\n",
      "       1, 3, 1, 2, 0, 3, 3, 1, 2, 3, 1, 2, 0, 2, 2, 3, 1, 0, 2, 1, 0, 2,\n",
      "       0, 3, 1, 3, 2, 2, 3, 2, 0, 2, 0, 3, 2, 1, 1, 0, 2, 0, 2, 1, 1, 2,\n",
      "       0, 1, 2, 2, 3, 3, 0, 3, 0, 2, 3, 1, 1, 2, 2, 0, 3, 2, 0, 0, 0, 2,\n",
      "       0, 2, 0, 0, 3, 0, 0, 2, 0], dtype=int8), 'len': 141, 'bpp': array([0., 0., 0., ..., 0., 0., 0.]), 'rfam_id': 'RF00050'}\n",
      "{'id': 'CM000437.1/2186482-2186362', 'seq': 'ATCGCCAGGAGATCAATTTCAAAATCGTACATGGTTCTTGCCTTTTACCAGAACCATCCGGGAATTGTCTTCAATACAAATGGTGTATCAACTAGTTTTTGGAAACCTTTCCACTTCAAAT', 'seq_int': array([0, 3, 1, 2, 1, 1, 0, 2, 2, 0, 2, 0, 3, 1, 0, 0, 3, 3, 3, 1, 0, 0,\n",
      "       0, 0, 3, 1, 2, 3, 0, 1, 0, 3, 2, 2, 3, 3, 1, 3, 3, 2, 1, 1, 3, 3,\n",
      "       3, 3, 0, 1, 1, 0, 2, 0, 0, 1, 1, 0, 3, 1, 1, 2, 2, 2, 0, 0, 3, 3,\n",
      "       2, 3, 1, 3, 3, 1, 0, 0, 3, 0, 1, 0, 0, 0, 3, 2, 2, 3, 2, 3, 0, 3,\n",
      "       1, 0, 0, 1, 3, 0, 2, 3, 3, 3, 3, 3, 2, 2, 0, 0, 0, 1, 1, 3, 3, 3,\n",
      "       1, 1, 0, 1, 3, 3, 1, 0, 0, 0, 3], dtype=int8), 'len': 121, 'bpp': array([0., 0., 0., ..., 0., 0., 0.]), 'rfam_id': 'RF00020'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "for rfam in rfams:\n",
    "    #print(np.load(os.path.join(rfam_dir, rfam, rfam+\".npy\"),allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ef40b1-0171-496f-a8fa-9b186b7a812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"runs/Masked2501\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fcef9ed-0b72-4587-afaa-4ddb840b949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RNARepLearn.modules import LinearEmbedding, RPINetEncoder, AttentionDecoder\n",
    "layers = []\n",
    "layers.append(RPINetEncoder(4, 32, 3, 3))\n",
    "layers.append(AttentionDecoder(32, 4))\n",
    "model = torch.nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f448cf9d-ad09-46ae-b1bb-e25d395e7c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RNARepLearn.train import MaskedTraining, AutoEncoder\n",
    "training = MaskedTraining(model, 3, 15, writer)\n",
    "#training = AutoEncoder(model, 3, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf5083fa-ddc9-4de8-9d53-b5a2d20be8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch    1/   3] [Batch    1/ 250] Loss:  2.46e+02 Nucleotide-Loss:  1.38e+00 Edge-Loss:  1.07e+02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/RNARepLearn/train.py:75\u001b[0m, in \u001b[0;36mMaskedTraining.run\u001b[0;34m(self, data_loader, val_loader)\u001b[0m\n\u001b[1;32m     72\u001b[0m batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 75\u001b[0m nucs, bpp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m node_accuracy_val\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mint\u001b[39m((nucs\u001b[38;5;241m.\u001b[39mcpu()[nuc_mask]\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m==\u001b[39mtrue_x[nuc_mask]\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum()) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(nuc_mask))\n\u001b[1;32m     77\u001b[0m edge_loss_val\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mint\u001b[39m(kl_loss(bpp\u001b[38;5;241m.\u001b[39mcpu()[nuc_mask]\u001b[38;5;241m.\u001b[39mlog() , torch\u001b[38;5;241m.\u001b[39mtensor(reconstruct_bpp(batch\u001b[38;5;241m.\u001b[39medge_index, true_edges, (\u001b[38;5;28mlen\u001b[39m(bpp),\u001b[38;5;28mlen\u001b[39m(bpp)))[nuc_mask]))\u001b[38;5;241m.\u001b[39mitem()))\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/RNARepLearn/modules.py:50\u001b[0m, in \u001b[0;36mRPINetEncoder.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m---> 50\u001b[0m     batch\u001b[38;5;241m.\u001b[39mx, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/tmp/nicolas.goedert_pyg/tmpogkwjzd_.py:21\u001b[0m, in \u001b[0;36mSequential_d15316.forward\u001b[0;34m(self, x, edge_index, batch, edge_weight)\u001b[0m\n\u001b[1;32m     19\u001b[0m x, (h, c) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_1(x)\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_2(x, edge_index, batch, edge_weight)\n\u001b[0;32m---> 21\u001b[0m x, (h, c) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_4(x, edge_index, batch, edge_weight)\n\u001b[1;32m     23\u001b[0m x, (h, c) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_5(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/RFAM/lib/python3.10/site-packages/torch/nn/modules/rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    773\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training.run(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ac70196-af68-47d1-9292-8f7ec3144876",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_loader))\n",
    "true_x = torch.clone(batch.x)\n",
    "true_edges = torch.clone(batch.edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fc44c30-5fe7-4910-a1d2-d0347007f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RNARepLearn.utils import mask_batch, reconstruct_bpp\n",
    "#nuc_mask, edge_mask = mask_batch(batch,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70b7366e-d01c-4757-bb1a-af966486fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.double()\n",
    "nucs, bpp = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f4c7e6c-ac8b-4ba7-a039-e71c31307984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 123, 3089])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpp_n, mask = to_dense_batch(bpp, batch.batch)\n",
    "bpp_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f944feca-8344-4af1-a53d-1db78f665221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RNARepLearn.losses import BppReconstructionLoss \n",
    "from torch_geometric.utils import to_dense_batch\n",
    "loss1 = torch.nn.KLDivLoss(reduction=\"sum\")\n",
    "loss2 = BppReconstructionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "312b2098-9d19-4673-891c-c470f8bf7e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25832.2134, dtype=torch.float64, grad_fn=<KlDivBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1(to_dense_batch(bpp, batch.batch)[0].log() , to_dense_batch(torch.tensor(reconstruct_bpp(batch.edge_index, true_edges, (len(bpp),len(bpp)))), batch.batch)[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "425796b6-4a76-44a7-9e7d-403528598e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25832.2134, dtype=torch.float64, grad_fn=<KlDivBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2(bpp , torch.tensor(reconstruct_bpp(batch.edge_index, true_edges, (len(bpp),len(bpp)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83978a80-f668-4cff-94c8-bdacce2be795",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bpp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbpp\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bpp' is not defined"
     ]
    }
   ],
   "source": [
    "bpp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RFAM",
   "language": "python",
   "name": "rfam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
